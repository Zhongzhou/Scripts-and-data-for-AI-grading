{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using chatAPI to grade student response. \n",
    "The code is simplified and cleaned using custom modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup all the packages\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "import re #this handles regular expression.\n",
    "import dill as pickle #pickle cannot handle functions, dill can. \n",
    "#import pipe\n",
    "\n",
    "load_dotenv() #loads the dotenv for API keys as environmental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binary_output #used to check or extract binary grading outcomes.\n",
    "import calc_price #calculate the price of each run based on price models.\n",
    "import pickle_tools #used to save and load a list of variables, avoids variables that cannot be serialized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = 'chatAPI_scriptCleaning' #This is used as the folder name to save pickled variables\n",
    "nRubricItems = 3 #How many items are there in the rubric, used for checking/extracting binary grading outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this chunck to load previously saved variables\n",
    "try:\n",
    "    pickle_tools.load_from_pickle(folderName=project_folder, globalVars = globals())\n",
    "except FileNotFoundError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load student responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_responses = pd.read_csv(\"./data/StudentResponse_Full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all the components of the grading prompt.\n",
    "For chat API, the prompt template currently contains two messages, a system message and a human message (in few shot learning it can contain multiple human and ai messages).\n",
    "The system message will just be a message for now (select a message from a list)\n",
    "The human message will be from a prompt template with the following variables:\n",
    "* Problem body\n",
    "* Rubric\n",
    "* Requirements\n",
    "* Student Response\n",
    "\n",
    "Note 7/22/2024: I'm currently not utilizing output formatting options as they are not necessary for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt message and template dictionary\n",
    "prompt_dict ={\n",
    "    'sys_messages': [\n",
    "        \"\"\"You are a college introductory level physics teacher who is grading a student's written explanation to a physics problem based on a grading rubric, following the given instruction.\"\"\",\n",
    "        \"\"\"You are a college introductory physics teacher who is grading a student's written explanation to a physics problem based on a grading rubric. Your grading always ends with a comma separated binary vector.\"\"\"\n",
    "    ],\n",
    "    'human_prompt_template': {\n",
    "        'no-formatting': {},\n",
    "        'with-formatting': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt_dict['human_prompt_template']['no-formatting'] =  \"\"\"Here is a college introductory level physics problem: \n",
    "\"{ProblemBody}\"\n",
    "Students are instructed to provide an explanation to their answer.\n",
    "Student explanations are being graded based on the following rubric:\n",
    "\"{Rubric}\"\n",
    "Grading is performed strictly according to the following requirements: \n",
    "# The grading must start with the evaluation of each individual rubric item.\n",
    "{Requirements}\n",
    "# For each rubric item, the student explanation will receive 1 point if the explanation satisfies the rubric, or 0 point if the explanation does not satisfy the rubric. Never assign a 0.5 for an item. \n",
    "# Each rubric item is graded only once.  \n",
    "# Steps or sentences in student's explanation may not follow the same order as the rubric. \n",
    "# Conclude the grading response with vector of length 3, included in curly brackets and separated by commas, such as {{0,0,0}} or {{1,0,1}} or {{1,1,1}}. The vector summarizes the grading of each of the three rubric items. \n",
    "Student response:\n",
    "\"{StudentResponse}\"\n",
    "Grading:\n",
    "\"\"\"\n",
    "\n",
    "#This \"with_formatting part is not needed here.\"\n",
    "prompt_dict['human_prompt_template']['with-formatting'] = \"\"\"Here is a college introductory level physics problem: \n",
    "\"{ProblemBody}\"\n",
    "Students are instructed to provide an explanation to their answer.\n",
    "Student explanations are being graded based on the following rubric:\n",
    "\"{Rubric}\"\n",
    "Grading is performed strictly according to the following requirements: \n",
    "# The grading must start with the evaluation of each individual rubric item.\n",
    "{Requirements}\n",
    "# For each rubric item, the student explanation will receive 1 point if the explanation satisfies the rubric, or 0 point if the explanation does not satisfy the rubric. Never assign a 0.5 for an item. \n",
    "# Each rubric item is graded only once.  \n",
    "# Steps or sentences in student's explanation may not follow the same order as the rubric. \n",
    "\n",
    "{Format_Instructions}\n",
    "\n",
    "Student response:\n",
    "\"{StudentResponse}\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ProblemBody', 'Requirements', 'Rubric', 'StudentResponse']\n"
     ]
    }
   ],
   "source": [
    "#create the langchain prompt template with four or five input variables\n",
    "#this one is without json output parsing. See if removing \"partial credit\" will stop it from giving 0.5s.\n",
    "prompt_template_noformatting = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt_dict['sys_messages'][1]), #use the system message for no output formatting.\n",
    "    (\"human\", prompt_dict['human_prompt_template']['no-formatting']) #use the no formatting human message template\n",
    "])\n",
    "print(prompt_template_noformatting.input_variables) #list the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are different versions of the rubric and grading requirements for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changeable components for problem body, rubric and requirements\n",
    "prompt_components_dict = {\n",
    "    'ProblemBody':\"\"\"Swimmers at a water park have a choice of two frictionless water slides. Both slides drop over the same height h: slide 1 is straight while slide 2 is curved, dropping quickly at first and then leveling out. How does the speed v1 of a swimmer reaching the bottom of slide 1 compare with v2, the speed of a swimmer reaching the end of slide 2?\"\"\",\n",
    "    'Rubric':{},\n",
    "    'Requirements':{}\n",
    "}\n",
    "\n",
    "prompt_components_dict['Rubric']['simple'] =\"\"\"\n",
    "# Item 1: The student should mention either one of the following: \n",
    "  * conservation of energy OR\n",
    "  * work and kinetic energy theorem \n",
    "\n",
    "# Item 2: The student mentioned either one of the following: \n",
    "   * No net external non-conservative work is being done, so mechanical energy  is conserved for the system  OR \n",
    "   * the slide is frictionless/smooth OR\n",
    "   * gravity is the only force that does work on the girl. \n",
    "\n",
    "# Item 3: The student indicated either one of the following: \n",
    "   * potential energy is converted into kinetic energy OR  \n",
    "   * Work done by gravity/gravitational force is equal to the change in kinetic energy of the girl\"\"\"\n",
    "\n",
    "prompt_components_dict['Rubric']['detailed']=\"\"\"\n",
    "# Item 1: \n",
    "\t\"* The student should mention either one of the following in the explanation: \n",
    "\t\t** Conservation of energy/mechanical energy. Conservation of Energy can be expressed in mathematical forms such as mgh = 1/2 mv^2, mghi+ ½mvi^2=mghf+ ½mvf^2, or MEi = MEf\n",
    "\t\t** work and kinetic energy theorem. \n",
    "\t\t** The student could explicitly mention both (gravitational) potential energy and kinetic energy, or mention both work and kinetic energy, without explicilty saying conservation of energy or name of the theorem.\n",
    "\t* Only mentioning the term potential energy will NOT satisfy this rubric. \n",
    "\t* The explanation cannot mention momentum, linear momentum, or centripetal forces\"\n",
    "\n",
    "# Item 2: \n",
    "\t\"* The student mentioned either one of the following in the explanation: \n",
    "\t\t** No net external non-conservative work is being done, so mechanical energy is conserved for the system. \n",
    "\t\t** the slide is frictionless, or that the slide is smooth, so that mechanical energy is conserved. Note that the explanation must have indicated using mechanical energy/enery/work principles.   \n",
    "\t\t** gravity is the only force that does work on the girl.\n",
    "\t\t** No non-conservative forces do work on the system.\"\n",
    "    \n",
    "# Item 3: \n",
    "\t* The student explanation indicated either one of the following: \n",
    "\t\t** potential energy or gravitational potential energy is converted or turned into kinetic energy. \n",
    "\t\t** Work done by gravity or gravitational force is equal to the change in kinetic energy of the girl or the swimmer. \n",
    "\t\t** Discussed relation between work done by gravity and kinetic energy or the velocity of the girl/swimmer.\n",
    "\t\t** Discussed the relation between potential energy or height, and the final kinetic energy (or the girl's velocity), when the rest of the explanation resolves around energy concepts.\n",
    "\t* The student can express potential energy as mgh or mgy, and kinetic energy as 1/2mv^2 or 0.5 mv^2. They can write expression such as mgh = 1/2mv^2\"\n",
    "    \"\"\"\n",
    "\n",
    "## I'm not sure what this \"detailed_2\" rubric is doing.\n",
    "prompt_components_dict['Rubric']['detailed_2']=\"\"\"\n",
    "# Item 1: \"The student should mention one of the following: \n",
    "\t * conservation of energy/mechanical energy\n",
    "     * work and kinetic energy theorem. \n",
    "    The student could also indicate that potential energy is converted or tranformed in to kinetic energy.\"\n",
    "\n",
    "# Item 2: \"The student mentioned one of the following: \n",
    "\t* No net external non-conservative work is being done, so mechanical energy is conserved for the system. \n",
    "\t* the slide is frictionless, or that the slide is smooth, so mechanical energy is conserved. Student's explanation must explicitly contain \"frictionless\" or \"smooth\" or a similar phrase\n",
    "\t* gravity is the only force that does work on the girl.\"\n",
    "    \n",
    "# Item 3: \"The student explanation indicated one of the following: \n",
    "\t* potential energy is converted into kinetic energy. The student must explicitly mention either gravitational potential energy is converted or turned into kinetic energy,\n",
    "\t* Work done by gravity or gravitational force is equal to the change in kinetic energy of the girl. \n",
    "    * Discussed relation between work done by gravity and kinetic energy of the girl.\"\n",
    "\"\"\"\n",
    "\n",
    "prompt_components_dict['Requirements']['test'] = \"\"\"# For each rubric item, first say the exact same words \"This is a test\", then give it a grade of 1.\"\"\"\n",
    "prompt_components_dict['Requirements']['naive_cot']=\"\"\"\n",
    "# For each rubric item, first write step by step reasoning on why or why not the student explanation satisfies or contradicts the item. Then assign a binary grade of either 0 or 1, with 1 indicating the student explanation satisfied the rubric item, and 0 otherwise.\n",
    "\"\"\"\n",
    "prompt_components_dict['Requirements']['comparison']=\"\"\"\n",
    "# For each rubric item, first compare student explanation with the rubric item description, then conclude if the explanation satisfies or didn't satisfy the rubric item. Finally, with 1 indicating the student explanation satisfied the rubric item, and 0 otherwise.\n",
    "\"\"\"\n",
    "\n",
    "prompt_components_dict['Requirements']['force_compare'] = \"\"\"\n",
    "# For each rubric item, write the grading statement strictly following the order of the statements below: \n",
    "  ## First, state one of the following two:  \n",
    "     \"For item <<item number>>, the rubric states that <<quote from the rubric item description>>. The most relevant parts in the student explanation are <<direct quote or quotes from student explanation>>.\n",
    "     \"For item <<item number>>, the rubric states that <<quote from the rubric item description>>. No part in the students' explanation is relevant to the rubric\"\n",
    "  ## then state one of the following: \n",
    "     \"the student explanation is similar to this part of the rubric description <<most similar part of the rubric>>\", \n",
    "     \"the student explanation and the rubric description are very different\" \n",
    "     \"the student explanation and the rubric description are irrelevant\"\n",
    "  ## Finally, conclude with a binary score: \n",
    "     \"so the grade is 1\"\n",
    "     \"so the grade is 0\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a dictionary that toggles the components to form grading condition dictionary that will be sent to the llm chain. Each element is a combination of a rubric style and a prompt style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grading_conditions_noparser = {\n",
    "    'naive_cot_1': {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['simple'],\n",
    "        'Requirements': prompt_components_dict['Requirements']['naive_cot']\n",
    "    },\n",
    "    'naive_cot_2': {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed'],\n",
    "        'Requirements': prompt_components_dict['Requirements']['naive_cot']\n",
    "    },\n",
    "    'detailed_compare' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['comparison']\n",
    "    },\n",
    "    'forced_compare' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['force_compare']\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the grading contion names for easier labeling of columns in the output dataframe\n",
    "grading_condition_names = list(grading_conditions_noparser.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_cot_1', 'naive_cot_2', 'detailed_compare', 'forced_compare']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_condition_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LLM communication and LLM chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the llm chain and test one grading.\n",
    "#setup the llm connection and basic parameters. \n",
    "#Empirically, temperatur 0.8 seems to be a good balance. This is of course a guess. \n",
    "#change to 800 tokens max to save some time and money based on previous experience.\n",
    "#only 4 stop sequences are allowed for Azure via API. This is an API issue\n",
    "llm_gpt35_chat = AzureChatOpenAI(    \n",
    "    api_version=\"2024-02-01\", #latest stable version\n",
    "    deployment_name = \"zchen-gpt35-chat\",\n",
    "    max_tokens = 1000,\n",
    "    temperature= 0.8,\n",
    "    #model_version = '0301',\n",
    "    model_kwargs= {\"stop\": [\"<|im_end|>\", \"Student response:\", \"Grading:\", \"Example \"]}\n",
    "    )\n",
    "\n",
    "#define the grading chain. If parsing is needed, it can be connected to the output parser\n",
    "grading_chain_gpt35 = prompt_template_noformatting | llm_gpt35_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function sends a response to llm for grading, and repeats the process until the outcome contains a binary string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to take the student response and grading input as parameters and invoke the chain to grade. \n",
    "def grade_by_llmChain(response: str, grading_input, chain, nItems = nRubricItems, problem = prompt_components_dict['ProblemBody']):\n",
    "    grading_input['StudentResponse'] = response # student response is the parameter to feed to the function. grading_input is from the grainding_condition list\n",
    "    grading_input['ProblemBody'] = problem #add the problem body\n",
    "    grading_output = chain.invoke(input=grading_input) #invoke the llm chain to produce the grading.\n",
    "    #check if the grading contains a binary output. If not, redo grading.\n",
    "    binaryPattern = binary_output.Create_Search(nItems) #using the binary outuput module to create search pattern\n",
    "    while not re.search(pattern=binaryPattern, string= grading_output.content):\n",
    "        print(\"proper grading output not found, re-do grading again.\")\n",
    "        grading_output = chain.invoke(input=grading_input)\n",
    "    return(grading_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract the grading text and grading outcome in one step.\n",
    "def extract_info(df : pd.DataFrame, outcomeName : str, nItems = nRubricItems):\n",
    "    df.loc[:,f'{outcomeName}_text'] = df.loc[:,outcomeName].apply(lambda x:x.content)\n",
    "    df.loc[:,f'{outcomeName}_grade'] = df.loc[:,f'{outcomeName}_text'].apply(lambda x:binary_output.Extract_binary(x, nItems= nItems))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function automates the grading and extract of information process to avoid code copying errors.\n",
    "def do_grading(promptStyle : str, llm_chain, response_df : pd.DataFrame, name_append = '', nItems = nRubricItems):\n",
    "    colName = promptStyle if name_append == \"\" else f\"{promptStyle}_{name_append}\"\n",
    "    response_df[colName] = response_df['response'].apply(grade_by_llmChain, grading_input = grading_conditions_noparser[promptStyle], chain = llm_chain, nItems = nItems)\n",
    "    response_df = extract_info(response_df, colName, nItems = nItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test case with one answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grading = grade_by_llmChain(\n",
    "    response=student_responses['response'][67],\n",
    "    chain = grading_chain_gpt35,\n",
    "    grading_input=grading_conditions_noparser['naive_cot_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(student_responses['response'][67])\n",
    "print(test_grading.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a random sample of 3 student answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_grading_test = student_responses.sample(3)\n",
    "batch_grading_test['naive_cot_2'] = batch_grading_test['response'].apply(grade_by_llmChain, chain = grading_chain_gpt35, grading_input = grading_conditions_noparser['naive_cot_2'])\n",
    "batch_grading_test = extract_info(batch_grading_test, 'naive_cot_2')\n",
    "#batch_grading_test['naive_cot_2_text'] = batch_grading_test['naive_cot_2'].apply(lambda x:x.content)\n",
    "print(batch_grading_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_grading_test.to_csv(\"./data_chatAPI/test_grading.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.006888"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the price of this test run\n",
    "calc_price.Calc_Price(batch_grading_test['naive_cot_2'], modelUsed= 'gpt35')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the grading outcome. This is useful for prompt engineering.\n",
    "batch_grading_test['naive_cot_2_text'].apply(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade all responses using GPT-35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this line only once as it creates a new variable\n",
    "# full_grading_gpt35 = student_responses.copy() #use copy to create a new variable with new variable id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grade all the student responses using gpt-35chat, naive_cot_2\n",
    "# Note: The grading process ran about 10 minutes.\n",
    "full_grading_gpt35['naive_cot_2'] = full_grading_gpt35['response'].apply(grade_by_llmChain, grading_input = grading_conditions_noparser['naive_cot_2'], chain = grading_chain_gpt35) #grade\n",
    "full_grading_gpt35 = extract_info(full_grading_gpt35, 'naive_cot_2')\n",
    "full_grading_gpt35.to_csv('./data_chatAPI/full_grading.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_price.Calc_Price(full_grading_gpt35['naive_cot_2'], modelUsed=\"gpt35\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'detailed_compare'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_condition_names[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt35, response_df= full_grading_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt35.to_csv(\"./data_chatAPI/full_grading.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below grades with gpt-4o model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the llm connection with gpt-4o. \n",
    "#empirically, temperatur 0.8 seems to be a good balance. This is of course a guess. \n",
    "#only 4 stop sequences are allowed for Azure via API. This is an API issue. Under chat API this seems to not be a problem.\n",
    "gpt4_model = AzureChatOpenAI(    \n",
    "    api_version=\"2024-02-01\", #latest stable version\n",
    "    deployment_name = \"zchen-test-gpt-4o\",\n",
    "    max_tokens = 1000,\n",
    "    temperature= 0.8,\n",
    "    #model_version = '0301',\n",
    "    model_kwargs= {\"stop\": [\"<|im_end|>\", \"Student response:\", \"Grading:\", \"Example \"]}\n",
    "    )\n",
    "\n",
    "#define the grading chain. If parsing is needed, it can be connected to the output parser\n",
    "grading_chain_gpt4o = prompt_template_noformatting | gpt4_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test grading of one answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's evaluate the student's response based on the three rubric items step by step.\n",
      "\n",
      "**Item 1:** The student should mention either one of the following: \n",
      "  * conservation of energy OR\n",
      "  * work and kinetic energy theorem \n",
      "\n",
      "- The student explicitly states that \"the Mechanical energy of the system is preserved all the way through,\" referencing the conservation of energy.\n",
      "- Therefore, the student's explanation satisfies the first rubric item.\n",
      "\n",
      "Grade for Item 1: 1\n",
      "\n",
      "**Item 2:** The student should mention either one of the following:\n",
      "   * No net external non-conservative work is being done, so mechanical energy is conserved for the system OR \n",
      "   * the slide is frictionless/smooth OR\n",
      "   * gravity is the only force that does work on the girl.\n",
      "\n",
      "- The student mentions \"the slide is frictionless,\" satisfying the requirement that the slide is frictionless/smooth.\n",
      "- Therefore, the student's explanation satisfies the second rubric item.\n",
      "\n",
      "Grade for Item 2: 1\n",
      "\n",
      "**Item 3:** The student should indicate either one of the following:\n",
      "   * potential energy is converted into kinetic energy OR  \n",
      "   * Work done by gravity/gravitational force is equal to the change in kinetic energy of the girl.\n",
      "\n",
      "- The student states, \"At the bottom of the slides, each swimmer has the same kinetic energy,\" which implies that the potential energy at the top has been converted into kinetic energy at the bottom.\n",
      "- Therefore, the student's explanation satisfies the third rubric item.\n",
      "\n",
      "Grade for Item 3: 1\n",
      "\n",
      "Based on the detailed evaluation, the student's explanation has satisfied all three rubric items.\n",
      "\n",
      "Final grading vector: {1,1,1}\n"
     ]
    }
   ],
   "source": [
    "test_grading_gpt4 = grade_by_llmChain(\n",
    "    response=student_responses.query('student == 57') ['response'].iloc[0],\n",
    "    grading_input=grading_conditions_noparser['naive_cot_1'],\n",
    "    chain= grading_chain_gpt4o\n",
    ")\n",
    "print(test_grading_gpt4.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test grading of multiple answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grading_gpt4o = student_responses.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grading_gpt4o['naive_cot_2'] = test_grading_gpt4o['response'].apply(grade_by_llmChain, grading_input = grading_conditions_noparser['naive_cot_2'], chain = grading_chain_gpt4o) #grade\n",
    "test_grading_gpt4o = extract_info(test_grading_gpt4o, 'naive_cot_2', nItems=3)\n",
    "#test_grading_gpt4o['naive_cot_2_text'] = test_grading_gpt4o['naive_cot_2'].apply(lambda x:x.content) #extract the content of the grading\n",
    "#test_grading_gpt4o['naive_cot_2_grade'] = test_grading_gpt4o['naive_cot_2_text'].apply(extract_binary_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for reproducibility\n",
    "test_grading_gpt4o.to_csv(\"./data_chatAPI/test_grading_gpt4_3response.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02874"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the cost of running\n",
    "calc_price.Calc_Price(test_grading_gpt4o['naive_cot_2'], 'gpt4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPT-4o to do full grading now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_grading_gpt4o = student_responses.copy() #Need to use the copy method to create a different variable, not an alias of the same variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt4o = pd.read_csv(\"./data_chatAPI/full_grading_gpt4o.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gpt-4o on Azure took 22m36s to produce grading for 96 student responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grading output not found, re-do grading again.\n",
      "grading output not found, re-do grading again.\n",
      "grading output not found, re-do grading again.\n",
      "grading output not found, re-do grading again.\n",
      "grading output not found, re-do grading again.\n",
      "grading output not found, re-do grading again.\n",
      "grading output not found, re-do grading again.\n",
      "grading output not found, re-do grading again.\n",
      "grading output not found, re-do grading again.\n",
      "grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "full_grading_gpt4o['naive_cot_2'] = full_grading_gpt4o['response'].apply(grade_by_llmChain, grading_input = grading_conditions_noparser['naive_cot_2'], chain = grading_chain_gpt4o, nItems = 3) #grade\n",
    "full_grading_gpt4o = extract_info(full_grading_gpt4o, 'naive_cot_2', nItems=3)\n",
    "#full_grading_gpt4o['naive_cot_2_text'] = full_grading_gpt4o['naive_cot_2'].apply(lambda x:x.content) #extract the content of the grading\n",
    "#full_grading_gpt4o['naive_cot_2_grade'] = full_grading_gpt4o['naive_cot_2_text'].apply(extract_binary_vector) #extract the binary vector in the grading.\n",
    "\n",
    "#save to csv file\n",
    "full_grading_gpt4o.to_csv(\"./data_chatAPI/full_grading_gpt4o.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[3], llm_chain=grading_chain_gpt4o, response_df= full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-4o takes significantly longer to grade. Check if I have a stricter rate limit quota on gpt-4o compared to gpt-35 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv file\n",
    "full_grading_gpt4o.to_csv(\"./data_chatAPI/full_grading_gpt4o.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing variables using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['student_responses', 'prompt_components_dict', 'prompt_dict', 'prompt_template_noformatting', 'grading_conditions_noparser', 'batch_grading_test', 'extract_info', 'grade_by_llmChain', 'full_grading_gpt35', 'test_grading_gpt4o', 'full_grading_gpt4o']\n"
     ]
    }
   ],
   "source": [
    "#use the globalVars parameter to allow the function to access the current global variables list.\n",
    "try:\n",
    "    pickle_tools.save_as_pickle(\n",
    "        variables=[student_responses, \n",
    "                   prompt_components_dict, \n",
    "                   prompt_dict, \n",
    "                   prompt_template_noformatting, \n",
    "                   grading_conditions_noparser, \n",
    "                   batch_grading_test, \n",
    "                   extract_info, \n",
    "                   grade_by_llmChain,\n",
    "                   full_grading_gpt35, \n",
    "                   test_grading_gpt4o,\n",
    "                   full_grading_gpt4o], \n",
    "        folderName = project_folder,\n",
    "        globalVars=globals())\n",
    "except IndexError as err:\n",
    "    print(err)\n",
    "\n",
    "print(pickled_varNames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
