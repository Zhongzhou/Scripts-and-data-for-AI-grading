{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using chatAPI to grade Q10 on the Final Exam\n",
    "The code is simplified and cleaned using custom modules, based on grading of Q8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup all the packages\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "import re #this handles regular expression.\n",
    "import dill as pickle #pickle cannot handle functions, dill can. \n",
    "#import pipe\n",
    "\n",
    "load_dotenv() #loads the dotenv for API keys as environmental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binary_output #used to check or extract binary grading outcomes.\n",
    "import calc_price #calculate the price of each run based on price models.\n",
    "import pickle_tools #used to save and load a list of variables, avoids variables that cannot be serialized.\n",
    "import print_response #used to print the response and grading in a very readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = 'Final_Q10' #This is used as the folder name to save pickled (serialized) variables\n",
    "nRubricItems = 3 #How many items are there in the rubric, used for checking/extracting binary grading outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(f\"./data_chatAPI/{project_folder}\"): \n",
    "    os.makedirs(f\"./data_chatAPI/{project_folder}\")\n",
    "    print(f\"created folder ./data_chatAPI/{project_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load key variables from previous session. \n",
    "If the chunck below is run, then there is no need to load data frames or lists. The GPT connections need to be re-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this chunck to load previously saved variables\n",
    "try:\n",
    "    pickle_tools.load_from_pickle(folderName=project_folder, globalVars = globals())\n",
    "except FileNotFoundError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load student responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_responses = pd.read_csv(\"./data/Final Q10/Q10-StudentResponse.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a random sample of 5 responses for prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this code if want a new set of random response. This is for prompt engineering.\n",
    "randomResponse_5 = student_responses.sample(5)\n",
    "randomResponse_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all the components of the grading prompt.\n",
    "For chat API, the prompt template currently contains two messages, a system message and a human message (in few shot learning it can contain multiple human and ai messages).\n",
    "The system message will just be a message for now (select a message from a list)\n",
    "The human message will be from a prompt template with the following variables:\n",
    "* Problem body\n",
    "* Rubric\n",
    "* Requirements\n",
    "* Student Response\n",
    "\n",
    "Note 7/22/2024: I'm currently not utilizing output formatting options as they are not necessary for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt message and template dictionary\n",
    "prompt_dict ={\n",
    "    'sys_messages': [\n",
    "        \"\"\"You are a college introductory level physics teacher who is grading a student's written explanation to a physics problem based on a grading rubric, following the given instruction.\"\"\",\n",
    "        \"\"\"You are a college introductory physics teacher who is grading a student's written explanation to a physics problem based on a grading rubric. Your grading always ends with a comma separated binary vector.\"\"\"\n",
    "    ],\n",
    "    'human_prompt_template': {\n",
    "        'no-formatting': {},\n",
    "        'with-formatting': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt_dict['human_prompt_template']['no-formatting'] =  \"\"\"Here is a college introductory level physics problem: \n",
    "\"{ProblemBody}\"\n",
    "Students are instructed to provide an explanation to their answer.\n",
    "Student explanations are being graded based on the following rubric:\n",
    "\"{Rubric}\"\n",
    "Grading is performed strictly according to the following requirements: \n",
    "# The grading must start with the evaluation of each individual rubric item.\n",
    "{Requirements}\n",
    "# For each rubric item, the student explanation will receive 1 point if the explanation satisfies the rubric, or 0 point if the explanation does not satisfy the rubric. Never assign a 0.5 for an item. \n",
    "# Each rubric item is graded only once.  \n",
    "# Steps or sentences in student's explanation may not follow the same order as the rubric. \n",
    "# Conclude the grading response with vector of length 3, included in curly brackets and separated by commas, such as {{0,0,0}} or {{1,0,1}} or {{1,1,1}}. The vector summarizes the grading of each of the three rubric items. \n",
    "Student response:\n",
    "\"{StudentResponse}\"\n",
    "Grading:\n",
    "\"\"\"\n",
    "\n",
    "#This \"with_formatting part\" is not needed here.\n",
    "prompt_dict['human_prompt_template']['with-formatting'] = \"\"\"Here is a college introductory level physics problem: \n",
    "\"{ProblemBody}\"\n",
    "Students are instructed to provide an explanation to their answer.\n",
    "Student explanations are being graded based on the following rubric:\n",
    "\"{Rubric}\"\n",
    "Grading is performed strictly according to the following requirements: \n",
    "# The grading must start with the evaluation of each individual rubric item.\n",
    "{Requirements}\n",
    "# For each rubric item, the student explanation will receive 1 point if the explanation satisfies the rubric, or 0 point if the explanation does not satisfy the rubric. Never assign a 0.5 for an item. \n",
    "# Each rubric item is graded only once.  \n",
    "# Steps or sentences in student's explanation may not follow the same order as the rubric. \n",
    "\n",
    "{Format_Instructions}\n",
    "\n",
    "Student response:\n",
    "\"{StudentResponse}\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ProblemBody', 'Requirements', 'Rubric', 'StudentResponse']\n"
     ]
    }
   ],
   "source": [
    "#create the langchain prompt template with four or five input variables\n",
    "#this one is without json output parsing. See if removing \"partial credit\" will stop it from giving 0.5s.\n",
    "prompt_template_noformatting = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt_dict['sys_messages'][1]), #use the system message for no output formatting.\n",
    "    (\"human\", prompt_dict['human_prompt_template']['no-formatting']) #use the no formatting human message template\n",
    "])\n",
    "print(prompt_template_noformatting.input_variables) #list the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are different versions of the rubric and grading requirements for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changeable components for problem body, rubric and requirements\n",
    "prompt_components_dict = {\n",
    "    'ProblemBody':\"\"\"A small massive ball of mass [m] kg is dropped straight down into a tube containing an ideal spring. The spring has spring constant k = [k] N/m and relaxed length of L0 = [L0] meters. The ball was launched to a height of h = [h] meters above the top of the spring. When the spring was compressed to a length of L = [L] meters, what is the magnitude of velocity v of the ball at that time in units of meters per second?\"\"\",\n",
    "    'Rubric':{},\n",
    "    'Requirements':{}\n",
    "}\n",
    "\n",
    "prompt_components_dict['Rubric']['simple'] =\"\"\"\n",
    "# Item 1: The student wrote down conservation of mechanical energy equation or indicated that mechanical energy can be used to solve the problem \n",
    "\n",
    "# Item 2: The potential energy term of the conservation of mechanical energy formula contains both a gravitational potential energy term and an elastic potential energy term.\n",
    "\n",
    "# Item 3: The gravitational potential energy term contains an expression similar to mg(h + L - L_0), and shouldn't be just mgh or mgL\n",
    "\"\"\"\n",
    "\n",
    "prompt_components_dict['Rubric']['detailed']=\"\"\"\n",
    "# Item 1: The student wrote down conservation of mechanical energy equation or indicated that mechanical energy can be used to solve the problem \n",
    "   * The student could write mathematical expressions such as MEi = MEf, ME_i - ME_f = 0, or KE_i + PE_i = KE_f + PE_f, or other similar forms  \n",
    "   * Students could use terms such as \"Energy\", or \"Mechanical Energy\". \n",
    " \n",
    "# Item 2: The potential energy term of the conservation of mechanical energy formula contains both a gravitational potential energy term and an elastic potential energy term.\n",
    "   * The student must mention both gravitational potential energy and elastic potential energy in the solution, mentioning only one of the two will not satisfy this rubric item.\n",
    "   * Elastic potential energy could be implied in mathematical expressions such as 1/2k(L-L0)^2, or 0.5*k*x^2, or 0.5*k(L0-L)^2.\n",
    "   * For this rubric item only, gravitational potential energy could be implied in mathematical forms consisting of mg multiplied by a height or distance measure, such as mgh, mg(L-L0), or mg(h-L-L0)\n",
    "\n",
    "# Item 3: The gravitational potential energy term contains an expression similar to mg(h + L - L_0), and shouldn't be just mgh or mgL\n",
    "   * The gravitational potential energy term could take forms such as mg(h + L0 - L) or m*g*(h-l+l0), or other forms that involves modifications to the height h.\n",
    "   * The student could also write expressions such as mgh + mg(L-L0) or mgh + mg*(L_0 - L) or similar forms\n",
    "   * Stating that gravitational potential energy is mgh, or including mgh alone will not satisfy this rubric item.\n",
    "\"\"\"\n",
    "\n",
    "## This is the improved detailed rubric based on reviewing error grades.\n",
    "## Major improvements to item 3: rubric item states the cognitive process, rather than using a specific representation of said operation.\n",
    "prompt_components_dict['Rubric']['detailed_2']=\"\"\"\n",
    "# Item 1: The student wrote down conservation of mechanical energy equation or indicated that mechanical energy can be used to solve the problem \n",
    "   * The student could write mathematical expressions such as MEi = MEf, ME_i - ME_f = 0, or KE_i + PE_i = KE_f + PE_f, or other similar forms  \n",
    "   * Students could use terms such as \"Energy\", or \"Mechanical Energy\". \n",
    "   * The student could also write the kinetic and potential energy terms, such as 0.5mv^2, 1/2kx^2, mgh, mg(h-L), or similar terms, without explicitly mentioning mechanical energy.\n",
    " \n",
    "# Item 2: The student solution included both a gravitational potential energy term and an elastic potential energy term.\n",
    "   * The student must mention both gravitational potential energy and elastic potential energy in the solution, mentioning only one of the two will not satisfy this rubric item.\n",
    "   * Elastic potential energy could be implied in mathematical expressions such as 1/2k(L-L0)^2, or 0.5*k*x^2, or 0.5*k(L0-L)^2. \n",
    "   * For this rubric item only, gravitational potential energy could be implied in mathematical forms consisting of mg multiplied by a height or distance measure, such as mgh, mg(L-L0), mg(h-L-L0), mg(h-L0)\n",
    "   * The final elastic potential energy is zero, so elastic potential energy can be omitted in the final potential energy of the situation.\n",
    "\n",
    "# Item 3: The student indicated that the calculation of the gravitational potential energy involves a modification to the height h.\n",
    "   * The gravitational potential energy term could take forms such as mg(h + L0 - L) or m*g*(h-l+l0), mg(h-L), mg(h+L0) or other forms that involves the term mg multiplied by a height that is not just h.\n",
    "   * The student could also write expressions such as mgh + mg(L-L0) or mgh + mg*(L_0 - L), or include both an mgh term and mg(l-l0) on different sides of the equation. \n",
    "   * The student could also indicate in words that the height in gravitational potential energy can be calculated by adding the spring compression length to the height of the ball. \n",
    "   * Stating that gravitational potential energy is just mgh, or including mgh alone in the equation will not satisfy this rubric item.\n",
    "\"\"\"\n",
    "\n",
    "#the 'test item is added here to verify that the code is correct and different prompts are acutally being passed to the LLM.\n",
    "prompt_components_dict['Requirements']['test'] = \"\"\"# For each rubric item, first say the exact same words \"This is a test\", then give it a grade of 1.\"\"\"\n",
    "prompt_components_dict['Requirements']['naive_cot']=\"\"\"\n",
    "# For each rubric item, first write step by step reasoning on why or why not the student explanation satisfies or contradicts the item. Then assign a binary grade of either 0 or 1, with 1 indicating the student explanation satisfied the rubric item, and 0 otherwise.\n",
    "\"\"\"\n",
    "prompt_components_dict['Requirements']['comparison']=\"\"\"\n",
    "# For each rubric item, first compare student explanation with the rubric item and the item description, then conclude if the explanation satisfies or didn't satisfy the rubric item. Finally, assign a binary grade of either 0 or 1, with 1 indicating the student explanation satisfied the rubric item, and 0 otherwise.\n",
    "\"\"\"\n",
    "\n",
    "prompt_components_dict['Requirements']['force_compare'] = \"\"\"\n",
    "# For each rubric item, write the grading statement strictly following the order of the statements below: \n",
    "  ## First, state one of the following two:  \n",
    "     \"For item <<item number>>, the rubric states that <<quote from the rubric item description>>. The most relevant parts in the student explanation are <<direct quote or quotes from student explanation>>.\n",
    "     \"For item <<item number>>, the rubric states that <<quote from the rubric item description>>. No part in the students' explanation is relevant to the rubric\"\n",
    "  ## then state one of the following: \n",
    "     \"the student explanation is similar to this part of the rubric description <<most similar part of the rubric>>\", \n",
    "     \"the student explanation and the rubric description are very different\" \n",
    "     \"the student explanation and the rubric description are irrelevant\"\n",
    "  ## Finally, conclude with a binary score: \n",
    "     \"so the grade is 1\"\n",
    "     \"so the grade is 0\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a dictionary that toggles the components to form grading condition dictionary that will be sent to the llm chain. Each element is a combination of a rubric style and a prompt style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grading_conditions_noparser = {\n",
    "    'naive_cot_1': {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['simple'],\n",
    "        'Requirements': prompt_components_dict['Requirements']['naive_cot']\n",
    "    },\n",
    "    'naive_cot_2': {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed'],\n",
    "        'Requirements': prompt_components_dict['Requirements']['naive_cot']\n",
    "    },\n",
    "    'detailed_compare' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['comparison']\n",
    "    },\n",
    "    'forced_compare' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['force_compare']\n",
    "    },\n",
    "    'detailed_compare_2' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed_2'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['comparison']\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the grading contion names for easier labeling of columns in the output dataframe\n",
    "grading_condition_names = list(grading_conditions_noparser.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_cot_1',\n",
       " 'naive_cot_2',\n",
       " 'detailed_compare',\n",
       " 'forced_compare',\n",
       " 'detailed_compare_2']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_condition_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LLM communication and LLM chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the llm chain and test one grading.\n",
    "#setup the llm connection and basic parameters. \n",
    "#Empirically, temperatur 0.8 seems to be a good balance. This is of course a guess. \n",
    "#change to 800 tokens max to save some time and money based on previous experience.\n",
    "#only 4 stop sequences are allowed for Azure via API. This is an API issue\n",
    "llm_gpt35_chat = AzureChatOpenAI(    \n",
    "    api_version=\"2024-02-01\", #latest stable version\n",
    "    deployment_name = \"zchen-gpt35-chat\",\n",
    "    max_tokens = 1000,\n",
    "    temperature= 0.8,\n",
    "    #model_version = '0301',\n",
    "    model_kwargs= {\"stop\": [\"<|im_end|>\", \"Student response:\", \"Grading:\", \"Example \"]}\n",
    "    )\n",
    "\n",
    "#define the grading chain. If parsing is needed, it can be connected to the output parser\n",
    "grading_chain_gpt35 = prompt_template_noformatting | llm_gpt35_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function sends a response to llm for grading, and repeats the process until the outcome contains a binary string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to take the student response and grading input as parameters and invoke the chain to grade. \n",
    "def grade_by_llmChain(response: str, grading_input, chain, nItems = nRubricItems, problem = prompt_components_dict['ProblemBody']):\n",
    "    grading_input['StudentResponse'] = response # student response is the parameter to feed to the function. grading_input is from the grainding_condition list\n",
    "    grading_input['ProblemBody'] = problem #add the problem body\n",
    "    grading_output = chain.invoke(input=grading_input) #invoke the llm chain to produce the grading.\n",
    "    #check if the grading contains a binary output. If not, redo grading.\n",
    "    binaryPattern = binary_output.Create_Search(nItems) #using the binary outuput module to create search pattern\n",
    "    while not re.search(pattern=binaryPattern, string= grading_output.content):\n",
    "        print(\"proper grading output not found, re-do grading again.\")\n",
    "        grading_output = chain.invoke(input=grading_input)\n",
    "    return(grading_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract the grading text and grading outcome in one step.\n",
    "def extract_info(df : pd.DataFrame, outcomeName : str, nItems = nRubricItems):\n",
    "    df.loc[:,f'{outcomeName}_text'] = df.loc[:,outcomeName].apply(lambda x:x.content)\n",
    "    df.loc[:,f'{outcomeName}_grade'] = df.loc[:,f'{outcomeName}_text'].apply(lambda x:binary_output.Extract_binary(x, nItems= nItems))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a random sample of 5 student answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_grading_test_gpt35 = randomResponse_5.copy()\n",
    "promptStyle = 'detailed_compare'\n",
    "batch_grading_test_gpt35[promptStyle] = batch_grading_test_gpt35['response'].apply(grade_by_llmChain, chain = grading_chain_gpt35, grading_input = grading_conditions_noparser[promptStyle])\n",
    "batch_grading_test_gpt35 = extract_info(batch_grading_test_gpt35, promptStyle)\n",
    "#print(batch_grading_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_grading_test_gpt35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response.print_gradingOutcome(batch_grading_test_gpt35, grading_colName = f\"{promptStyle}_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01088"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the price of this test run\n",
    "calc_price.Calc_Price(batch_grading_test_gpt35[promptStyle], modelUsed= 'gpt35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function automates the grading and extract of information process to avoid code copying errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_grading(promptStyle : str, llm_chain, response_df : pd.DataFrame, name_append = '', nItems = nRubricItems):\n",
    "    colName = promptStyle if name_append == \"\" else f\"{promptStyle}_{name_append}\"\n",
    "    response_df[colName] = response_df['response'].apply(grade_by_llmChain, grading_input = grading_conditions_noparser[promptStyle], chain = llm_chain, nItems = nItems)\n",
    "    response_df = extract_info(response_df, colName, nItems = nItems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[1], llm_chain=grading_chain_gpt35, response_df= batch_grading_test_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_grading_test_gpt35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_grading_test_gpt35.to_csv(f\"./data_chatAPI/{project_folder}/grading_test_gpt35.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[4], llm_chain=grading_chain_gpt35, response_df= batch_grading_test_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_grading_test_gpt35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response.print_gradingOutcome(batch_grading_test_gpt35,'detailed_compare_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade all responses using GPT-35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this line only once as it creates a new variable\n",
    "#full_grading_gpt35 = student_responses.copy() #use copy to create a new variable with new variable id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_cot_1', 'naive_cot_2', 'detailed_compare', 'forced_compare']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_condition_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "#naive COT\n",
    "do_grading(promptStyle=grading_condition_names[0], llm_chain=grading_chain_gpt35, response_df= full_grading_gpt35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive COT took 8m 26.7s to grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14133"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_price.Calc_Price(full_grading_gpt35[grading_condition_names[0]], modelUsed=\"gpt35\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt35.to_csv(f\"./data_chatAPI/{project_folder}/full_grading_gpt35.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[1], llm_chain=grading_chain_gpt35, response_df= full_grading_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.188128"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_price.Calc_Price(full_grading_gpt35[grading_condition_names[1]], modelUsed=\"gpt35\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt35.to_csv(f\"./data_chatAPI/{project_folder}/full_grading_gpt35.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt35, response_df= full_grading_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_price.Calc_Price(full_grading_gpt35[grading_condition_names[2]], modelUsed=\"gpt35\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[3], llm_chain=grading_chain_gpt35, response_df= full_grading_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt35.to_csv(f\"./data_chatAPI/{project_folder}/full_grading_gpt35.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below grades with gpt-4o model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the llm connection with gpt-4o. \n",
    "#empirically, temperatur 0.8 seems to be a good balance. This is of course a guess. \n",
    "#only 4 stop sequences are allowed for Azure via API. This is an API issue. Under chat API this seems to not be a problem.\n",
    "gpt4_model = AzureChatOpenAI(    \n",
    "    api_version=\"2024-02-01\", #latest stable version\n",
    "    deployment_name = \"zchen-test-gpt-4o\",\n",
    "    max_tokens = 1000,\n",
    "    temperature= 0.8,\n",
    "    #model_version = '0301',\n",
    "    model_kwargs= {\"stop\": [\"<|im_end|>\", \"Student response:\", \"Grading:\", \"Example \"]}\n",
    "    )\n",
    "\n",
    "#define the grading chain. If parsing is needed, it can be connected to the output parser\n",
    "grading_chain_gpt4o = prompt_template_noformatting | gpt4_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test grading of multiple answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grading_gpt4o = randomResponse_5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_cot_1', 'naive_cot_2', 'detailed_compare', 'forced_compare']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_condition_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptStyle = grading_condition_names[2] # detailed_compare\n",
    "test_grading_gpt4o[promptStyle] = test_grading_gpt4o['response'].apply(grade_by_llmChain, grading_input = grading_conditions_noparser[promptStyle], chain = grading_chain_gpt4o, nItems = nRubricItems) #grade\n",
    "test_grading_gpt4o = extract_info(test_grading_gpt4o, promptStyle, nItems=nRubricItems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grading_gpt4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response.print_gradingOutcome(test_grading_gpt4o, grading_colName=f'{promptStyle}_text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-4o did a really good job of grading the 5 response. All of them are actually correct, even for the edge case where the verbal expression is confusing.\n",
    "However, it took 29 seconds (as opposed to 7.9s) and costs 4 times as much as GPT 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for reproducibility\n",
    "test_grading_gpt4o.to_csv(f\"./data_chatAPI/{project_folder}/grading_test_gpt4o.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04315"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the cost of running\n",
    "#Note: I need a utility to calculate the price of multiple runs, and output a list\n",
    "calc_price.Calc_Price(test_grading_gpt4o['detailed_compare'], 'gpt4o')\n",
    "#calc_price.Calc_Price(test_grading_gpt4o['forced_compare'], 'gpt4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the identified more problematic items as a test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_items = pd.read_csv(\"./data_chatAPI/Final_Q10/diff_both_low_entropy.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_items['response'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Let\\'s evaluate the student response against each rubric item:\\n\\n### Item 1: Conservation of Mechanical Energy\\nThe student wrote down the kinetic energy and potential energy terms, such as 0.5mv^2, 1/2kx^2, and mgh. Although the student did not explicitly mention \"Mechanical Energy\" or write an equation such as MEi = MEf, the use of these terms indicates an understanding of energy conservation.\\n- **Satisfies Item 1:** Yes\\n- **Binary Score:** 1\\n\\n### Item 2: Inclusion of Both Gravitational and Elastic Potential Energy\\nThe student included both gravitational potential energy (PE_g = mgh) and elastic potential energy (PE_s = 1/2k(L-L0)^2) terms in their explanation.\\n- **Satisfies Item 2:** Yes\\n- **Binary Score:** 1\\n\\n### Item 3: Modification to the Height in Gravitational Potential Energy\\nThe student calculated the gravitational potential energy as mgh without modifying the height based on the compression of the spring. The term used is just mgh (6.05), which does not incorporate the change in height due to the spring compression.\\n- **Satisfies Item 3:** No\\n- **Binary Score:** 0\\n\\nThe final grading vector based on the rubric is {1, 1, 0}.', response_metadata={'token_usage': {'completion_tokens': 282, 'prompt_tokens': 1033, 'total_tokens': 1315}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_67802d9a6d', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}, id='run-f741a9bb-7edc-4b1a-a4f0-7399b1e77d3f-0', usage_metadata={'input_tokens': 1033, 'output_tokens': 282, 'total_tokens': 1315})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade_by_llmChain(problematic_items['response'][10], grading_input= grading_conditions_noparser['detailed_compare_2'], chain= grading_chain_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'detailed_compare_2'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_condition_names[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[4], llm_chain=grading_chain_gpt4o, response_df=problematic_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problematic_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response.print_gradingOutcome(problematic_items, \"detailed_compare_2_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPT-4o to do full grading now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following line is run only once at the initiation of the process\n",
    "#full_grading_gpt4o = student_responses.copy() #Need to use the copy method to create a different variable, not an alias of the same variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[0], llm_chain=grading_chain_gpt4o, response_df=full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[1], llm_chain=grading_chain_gpt4o, response_df=full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7623699999999999"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_price.Calc_Price(full_grading_gpt4o[grading_condition_names[0]], modelUsed='gpt4o')\n",
    "calc_price.Calc_Price(full_grading_gpt4o[grading_condition_names[1]], modelUsed='gpt4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-4o took 18 minutes to perform teach round of grading, and costs a little less than a dollar for each run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt4o, response_df= full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[3], llm_chain=grading_chain_gpt4o, response_df= full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[4], llm_chain=grading_chain_gpt4o, response_df= full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7608900000000001"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_price.Calc_Price(full_grading_gpt4o['detailed_compare_2'], modelUsed='gpt4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save to csv file\n",
    "full_grading_gpt4o.to_csv(f\"./data_chatAPI/{project_folder}/full_grading_gpt4o.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-consistency run with detailed compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_consistency = student_responses.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "#do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_1\") #first run\n",
    "#do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_2\") #second run \n",
    "#do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_3\") #third run\n",
    "#do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_4\") #fourth run\n",
    "#do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_5\") #fifth run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do detailed compare rubric 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "#do_grading(promptStyle=grading_condition_names[4], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_1\") #first run\n",
    "#do_grading(promptStyle=grading_condition_names[4], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_2\") #second run \n",
    "#do_grading(promptStyle=grading_condition_names[4], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_3\") #third run\n",
    "#do_grading(promptStyle=grading_condition_names[4], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_4\") #fourth run\n",
    "#do_grading(promptStyle=grading_condition_names[4], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_5\") #fifth run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_consistency.to_csv(f\"./data_chatAPI/{project_folder}/self_consistency_grading.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing variables using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['student_responses', 'randomResponse_5', 'prompt_components_dict', 'prompt_dict', 'prompt_template_noformatting', 'grading_conditions_noparser', 'grading_condition_names', 'do_grading', 'extract_info', 'batch_grading_test_gpt35', 'full_grading_gpt35', 'test_grading_gpt4o', 'problematic_items', 'full_grading_gpt4o', 'self_consistency']\n"
     ]
    }
   ],
   "source": [
    "#use the globalVars parameter to allow the function to access the current global variables list.\n",
    "try:\n",
    "    pickle_tools.save_as_pickle(\n",
    "        variables=[student_responses, \n",
    "                   randomResponse_5, \n",
    "                   prompt_components_dict, \n",
    "                   prompt_dict, \n",
    "                   prompt_template_noformatting, \n",
    "                   grading_conditions_noparser, \n",
    "                   grading_condition_names,\n",
    "                   do_grading, \n",
    "                   extract_info,\n",
    "                   batch_grading_test_gpt35,\n",
    "                   full_grading_gpt35, \n",
    "                   test_grading_gpt4o,\n",
    "                   problematic_items,\n",
    "                   full_grading_gpt4o,\n",
    "                   self_consistency\n",
    "                   ], \n",
    "        folderName = project_folder,\n",
    "        globalVars=globals())\n",
    "except IndexError as err:\n",
    "    print(err)\n",
    "\n",
    "print(pickled_varNames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
