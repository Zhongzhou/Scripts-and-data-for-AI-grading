{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using chatAPI to grade Q8 on the Final Exam\n",
    "The code is simplified and cleaned using custom modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#setup all the packages\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "import pandas as pd\n",
    "import re #this handles regular expression.\n",
    "import dill as pickle #pickle cannot handle functions, dill can. \n",
    "#import pipe\n",
    "\n",
    "load_dotenv() #loads the dotenv for API keys as environmental variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binary_output #used to check or extract binary grading outcomes.\n",
    "import calc_price #calculate the price of each run based on price models.\n",
    "import pickle_tools #used to save and load a list of variables, avoids variables that cannot be serialized.\n",
    "import print_response #used to print the response and grading in a very readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = 'FinalQ8' #This is used as the folder name to save pickled (serialized) variables\n",
    "nRubricItems = 3 #How many items are there in the rubric, used for checking/extracting binary grading outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load key variables from previous session. \n",
    "If the chunck below is run, then there is no need to load data frames or lists. The GPT connections need to be re-defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this chunck to load previously saved variables\n",
    "try:\n",
    "    pickle_tools.load_from_pickle(folderName=project_folder, globalVars = globals())\n",
    "except FileNotFoundError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load student responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    student  outcome                                           response\n",
      "0    292286        0  found the x momentum originally\\n\\nfound the y...\n",
      "1    379046        0  for this question it took me about 5 steps to ...\n",
      "2    545257        0  I'm solving for the final magnitude ofÂ  veloc...\n",
      "3    108612        0  This problem took about 4 steps.\\n\\nThe proble...\n",
      "4    842697        0  For each boulder respectively I multiplied the...\n",
      "..      ...      ...                                                ...\n",
      "72   714706        1  1) The problem was asking for the final veloci...\n",
      "73   671085        1  We're given the mass and velocities of both ob...\n",
      "74   177232        1  Since the boulders stick together, we must cal...\n",
      "75   953782        0  I solved this problem by taking the velocities...\n",
      "76   105981        1  Solving for v_f, we are given m1, m2, v1, v2, ...\n",
      "\n",
      "[77 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#Note: This is student responses without the alternate solution\n",
    "student_responses = pd.read_csv(\"./data/Final Q8/Q8-StudentResponse-Small.csv\")\n",
    "print(student_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a random sample of 5 responses for prompt engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this code if want a new set of random response. This is for prompt engineering.\n",
    "randomResponse_5 = student_responses.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomResponse_5 = batch_grading_test[['student', 'response', 'outcome']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup all the components of the grading prompt.\n",
    "For chat API, the prompt template currently contains two messages, a system message and a human message (in few shot learning it can contain multiple human and ai messages).\n",
    "The system message will just be a message for now (select a message from a list)\n",
    "The human message will be from a prompt template with the following variables:\n",
    "* Problem body\n",
    "* Rubric\n",
    "* Requirements\n",
    "* Student Response\n",
    "\n",
    "Note 7/22/2024: I'm currently not utilizing output formatting options as they are not necessary for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt message and template dictionary\n",
    "prompt_dict ={\n",
    "    'sys_messages': [\n",
    "        \"\"\"You are a college introductory level physics teacher who is grading a student's written explanation to a physics problem based on a grading rubric, following the given instruction.\"\"\",\n",
    "        \"\"\"You are a college introductory physics teacher who is grading a student's written explanation to a physics problem based on a grading rubric. Your grading always ends with a comma separated binary vector.\"\"\"\n",
    "    ],\n",
    "    'human_prompt_template': {\n",
    "        'no-formatting': {},\n",
    "        'with-formatting': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "prompt_dict['human_prompt_template']['no-formatting'] =  \"\"\"Here is a college introductory level physics problem: \n",
    "\"{ProblemBody}\"\n",
    "Students are instructed to provide an explanation to their answer.\n",
    "Student explanations are being graded based on the following rubric:\n",
    "\"{Rubric}\"\n",
    "Grading is performed strictly according to the following requirements: \n",
    "# The grading must start with the evaluation of each individual rubric item.\n",
    "{Requirements}\n",
    "# For each rubric item, the student explanation will receive 1 point if the explanation satisfies the rubric, or 0 point if the explanation does not satisfy the rubric. Never assign a 0.5 for an item. \n",
    "# Each rubric item is graded only once.  \n",
    "# Steps or sentences in student's explanation may not follow the same order as the rubric. \n",
    "# Conclude the grading response with vector of length 3, included in curly brackets and separated by commas, such as {{0,0,0}} or {{1,0,1}} or {{1,1,1}}. The vector summarizes the grading of each of the three rubric items. \n",
    "Student response:\n",
    "\"{StudentResponse}\"\n",
    "Grading:\n",
    "\"\"\"\n",
    "\n",
    "#This \"with_formatting part is not needed here.\"\n",
    "prompt_dict['human_prompt_template']['with-formatting'] = \"\"\"Here is a college introductory level physics problem: \n",
    "\"{ProblemBody}\"\n",
    "Students are instructed to provide an explanation to their answer.\n",
    "Student explanations are being graded based on the following rubric:\n",
    "\"{Rubric}\"\n",
    "Grading is performed strictly according to the following requirements: \n",
    "# The grading must start with the evaluation of each individual rubric item.\n",
    "{Requirements}\n",
    "# For each rubric item, the student explanation will receive 1 point if the explanation satisfies the rubric, or 0 point if the explanation does not satisfy the rubric. Never assign a 0.5 for an item. \n",
    "# Each rubric item is graded only once.  \n",
    "# Steps or sentences in student's explanation may not follow the same order as the rubric. \n",
    "\n",
    "{Format_Instructions}\n",
    "\n",
    "Student response:\n",
    "\"{StudentResponse}\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ProblemBody', 'Requirements', 'Rubric', 'StudentResponse']\n"
     ]
    }
   ],
   "source": [
    "#create the langchain prompt template with four or five input variables\n",
    "#this one is without json output parsing. See if removing \"partial credit\" will stop it from giving 0.5s.\n",
    "prompt_template_noformatting = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", prompt_dict['sys_messages'][1]), #use the system message for no output formatting.\n",
    "    (\"human\", prompt_dict['human_prompt_template']['no-formatting']) #use the no formatting human message template\n",
    "])\n",
    "print(prompt_template_noformatting.input_variables) #list the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are different versions of the rubric and grading requirements for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changeable components for problem body, rubric and requirements\n",
    "prompt_components_dict = {\n",
    "    'ProblemBody':\"\"\"Two icy boulders in Saturn's rings approach each other, collide, and stick together as shown in the figure below. The first has a mass of [m1] kg and velocity of [v1] m/s.The second has a mass of [m2] kg and velocity of [v2] LaTeX: m/s. The angle between the two velocities is theta. Determine the magnitude of their velocity after they collide. Round your answer to the nearest 1 decimal place in units of m/s.\"\"\",\n",
    "    'Rubric':{},\n",
    "    'Requirements':{}\n",
    "}\n",
    "\n",
    "prompt_components_dict['Rubric']['simple'] =\"\"\"\n",
    "# Item 1: The student solution decomposed the initial linear momentum of boulder 2 into its x and y components. \n",
    "\n",
    "# Item 2: The student wrote down conservation of linear momentum equation for both the x and y directions independently.\n",
    "\n",
    "# Item 3: The student used Pythagorean theorem to find the magnitude of the final velocity.\n",
    "\"\"\"\n",
    "\n",
    "prompt_components_dict['Rubric']['detailed']=\"\"\"\n",
    "# Item 1: The student considered the x and y components of the linear momentum of the second boulder separately. \n",
    "   * The student could write down the x and y components of linear momentum of the second boulder in two separate conservation of linear momentum equations. \n",
    "   * The student could decompose the linear momentum of the second boulder using trigonometry, for example m_2v2cos(theta), m2v2sin(theta), or mvcos(theta), mvsin(theta) for the second boulder. \n",
    "   * The student could explicitly state decomposing linear momentum of the second boulder into its x and y components.\n",
    "   * The student may also decompose the velocity of the second boulder into its x and y components.\n",
    " \n",
    "# Item 2: The student wrote down conservation of linear momentum equations for both the x and y directions independently.\n",
    "   * The student must write down equations for both x and y directions.\n",
    "   * Conservation of linear momentum equations can take forms such as m1v1x + m2v2x = (m1 + m2)vx, and m2v2y = (m1 + m2)vy.\n",
    "   * The student could imply that linear momentum is conserved on both x and y directions, or conservation of linear momentum procedure is applied to both directions.\n",
    "   * Only stating that linear momentum is conserved or writing down a single equation such as m1v1 + m2v2 = (m1 + m2)V do not satisfy this rubric.\n",
    "\n",
    "# Item 3: The student used Pythagorean theorem to find the magnitude of the final velocity.\n",
    "   * The student could write equations such as v_2f^2 = v_2xf^2 + v_2yf^2, or v2f = sqrt(v2xf^2 + v2yf^2)\n",
    "   * The student could also state that the final velocity is obtained using the pythagorean theorem, or by taking the square root of the velocity squares.\n",
    "   * The student could also state that the final velocity is the vector sum of the x component and y component velocities.\n",
    "   * The student could also apply pythagorean theorem directly to the linear momentums, and divide the final momentum by the mass of the boulders.\n",
    "   * The student could write equations such as (m2v2)^2 = (m2v2x)^2 + (m2v2y)^2\n",
    "\"\"\"\n",
    "\n",
    "## This is the improved detailed rubric based on reviewing error grades.\n",
    "## Major improvements to item 2 and 3.\n",
    "prompt_components_dict['Rubric']['detailed_2']=\"\"\"\n",
    "# Item 1: The student considered the x and y components of the linear momentum of the second boulder separately. \n",
    "   * The student could write down the x and y components of linear momentum of the second boulder in two separate conservation of linear momentum equations. \n",
    "   * The student could decompose the linear momentum of the second boulder using trigonometry, for example m_2v2cos(theta), m2v2sin(theta), or mvcos(theta), mvsin(theta) for the second boulder. \n",
    "   * The student could explicitly state decomposing linear momentum of the second boulder into its x and y components.\n",
    "   * The student could also decompose the velocity of the second boulder into its x and y components.\n",
    " \n",
    "# Item 2: The student applied conservation of linear momentum to both the x and y directions independently.\n",
    "   * Conservation of linear momentum equations can take forms such as m1v1x + m2v2x = (m1 + m2)vx, and m2v2y = (m1 + m2)vy.\n",
    "   * The student could imply that linear momentum is conserved on both x and y directions, or conservation of linear momentum procedure is applied to both directions.\n",
    "   * Only stating that linear momentum is conserved or writing down a single equation such as m1v1 + m2v2 = (m1 + m2)V do not satisfy this rubric.\n",
    "   * The student could write down one conservation of linear momentum equation, and either immediately or later indicate that this equation is applied to both x and y directions.\n",
    "\n",
    "# Item 3: The student used Pythagorean theorem to find the magnitude of the final velocity.\n",
    "   * The student could write equations such as v_2f^2 = v_2xf^2 + v_2yf^2, or v2f = sqrt(v2xf^2 + v2yf^2)\n",
    "   * The student could also state that the final velocity is obtained using the pythagorean theorem, or by taking the square root of the velocity squares.\n",
    "   * The student could also state that the final velocity is the vector sum of the x component and y component velocities.\n",
    "   * The student could write equations such as (m2v2)^2 = (m2v2x)^2 + (m2v2y)^2\n",
    "   * The student could also apply pythagorean theorem directly to the linear momentums, and divide the final momentum by the mass of the boulders.\n",
    "   * The student cannot apply pythagorean theorem directly to p1 and p2, or the linear momentum of boulders 1 and 2. The pythagorean theorem must by applied to the components of velocity or momentum.\n",
    "   * Simply stating \"obtain the magnitude\" of the velocity do not satisfy this rubric. \n",
    "\"\"\"\n",
    "\n",
    "prompt_components_dict['Rubric']['detailed_3']=\"\"\"\n",
    "# Item 1: The student solution decomposed the initial linear momentum of boulder 2 into its x and y components. \n",
    "   * The student could also write m2v2x and m2v2y in conservation of linear momentum equations without explicitly decomposing the momentum or the velocities.\n",
    "\n",
    "# Item 2: The student wrote down conservation of linear momentum equation for both the x and y directions independently.\n",
    "   * The student could also imply that linear momentum is conserved on both x and y directions separately, and may not explicitly write down conservation equations.\n",
    "   * Only saying \"momentum equation\" or \"used momentum\" does not satisfy this rubric.\n",
    "\n",
    "# Item 3: The student used Pythagorean theorem to find the magnitude of the final velocity.\n",
    "   * Just saying \"put them together\" does not satisfy this rubric\n",
    "\"\"\"\n",
    "\n",
    "#the 'test item is added here to verify that the code is correct and different prompts are actually being passed to the LLM.\n",
    "prompt_components_dict['Requirements']['test'] = \"\"\"# For each rubric item, first say the exact same words \"This is a test\", then give it a grade of 1.\"\"\"\n",
    "prompt_components_dict['Requirements']['naive_cot']=\"\"\"\n",
    "# For each rubric item, first write step by step reasoning on why or why not the student explanation satisfies or contradicts the item. Then assign a binary grade of either 0 or 1, with 1 indicating the student explanation satisfied the rubric item, and 0 otherwise.\n",
    "\"\"\"\n",
    "prompt_components_dict['Requirements']['comparison']=\"\"\"\n",
    "# For each rubric item, first compare student explanation with the rubric item and the item description, then conclude if the explanation satisfies or didn't satisfy the rubric item. Finally, assign a binary grade of either 0 or 1, with 1 indicating the student explanation satisfied the rubric item, and 0 otherwise.\n",
    "\"\"\"\n",
    "\n",
    "prompt_components_dict['Requirements']['force_compare'] = \"\"\"\n",
    "# For each rubric item, write the grading statement strictly following the order of the statements below: \n",
    "  ## First, state one of the following two:  \n",
    "     \"For item <<item number>>, the rubric states that <<quote from the rubric item description>>. The most relevant parts in the student explanation are <<direct quote or quotes from student explanation>>.\n",
    "     \"For item <<item number>>, the rubric states that <<quote from the rubric item description>>. No part in the students' explanation is relevant to the rubric\"\n",
    "  ## then state one of the following: \n",
    "     \"the student explanation is similar to this part of the rubric description <<most similar part of the rubric>>\", \n",
    "     \"the student explanation and the rubric description are very different\" \n",
    "     \"the student explanation and the rubric description are irrelevant\"\n",
    "  ## Finally, conclude with a binary score: \n",
    "     \"so the grade is 1\"\n",
    "     \"so the grade is 0\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a dictionary that toggles the components to form grading condition dictionary that will be sent to the llm chain. Each element is a combination of a rubric style and a prompt style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "grading_conditions_noparser = {\n",
    "    'naive_cot_1': {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['simple'],\n",
    "        'Requirements': prompt_components_dict['Requirements']['naive_cot']\n",
    "    },\n",
    "    'naive_cot_2': {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed'],\n",
    "        'Requirements': prompt_components_dict['Requirements']['naive_cot']\n",
    "    },\n",
    "    'detailed_compare' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['comparison']\n",
    "    },\n",
    "    'forced_compare' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['force_compare']\n",
    "    },\n",
    "    'detailed_compare_2' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed_2'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['comparison']\n",
    "    },\n",
    "    'detailed_compare_3' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['simple'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['comparison']\n",
    "    },\n",
    "    'naive_cot_3' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed_3'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['naive_cot']\n",
    "    },\n",
    "    'detailed_compare_4' : {\n",
    "        'Rubric' : prompt_components_dict['Rubric']['detailed_3'],\n",
    "        'Requirements' : prompt_components_dict['Requirements']['comparison']\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the grading contion names for easier labeling of columns in the output dataframe\n",
    "grading_condition_names = list(grading_conditions_noparser.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_cot_1',\n",
       " 'naive_cot_2',\n",
       " 'detailed_compare',\n",
       " 'forced_compare',\n",
       " 'detailed_compare_2',\n",
       " 'detailed_compare_3',\n",
       " 'naive_cot_3',\n",
       " 'detailed_compare_4']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_condition_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LLM communication and LLM chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the llm chain and test one grading.\n",
    "#setup the llm connection and basic parameters. \n",
    "#Empirically, temperatur 0.8 seems to be a good balance. This is of course a guess. \n",
    "#change to 800 tokens max to save some time and money based on previous experience.\n",
    "#only 4 stop sequences are allowed for Azure via API. This is an API issue\n",
    "llm_gpt35_chat = AzureChatOpenAI(    \n",
    "    api_version=\"2024-02-01\", #latest stable version\n",
    "    deployment_name = \"zchen-gpt35-chat\",\n",
    "    max_tokens = 1000,\n",
    "    temperature= 0.8,\n",
    "    #model_version = '0301',\n",
    "    model_kwargs= {\"stop\": [\"<|im_end|>\", \"Student response:\", \"Grading:\", \"Example \"]}\n",
    "    )\n",
    "\n",
    "#define the grading chain. If parsing is needed, it can be connected to the output parser\n",
    "grading_chain_gpt35 = prompt_template_noformatting | llm_gpt35_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function sends a response to llm for grading, and repeats the process until the outcome contains a binary string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function to take the student response and grading input as parameters and invoke the chain to grade. \n",
    "def grade_by_llmChain(response: str, grading_input, chain, nItems = nRubricItems, problem = prompt_components_dict['ProblemBody']):\n",
    "    grading_input['StudentResponse'] = response # student response is the parameter to feed to the function. grading_input is from the grainding_condition list\n",
    "    grading_input['ProblemBody'] = problem #add the problem body\n",
    "    grading_output = chain.invoke(input=grading_input) #invoke the llm chain to produce the grading.\n",
    "    #check if the grading contains a binary output. If not, redo grading.\n",
    "    binaryPattern = binary_output.Create_Search(nItems) #using the binary outuput module to create search pattern\n",
    "    while not re.search(pattern=binaryPattern, string= grading_output.content):\n",
    "        print(\"proper grading output not found, re-do grading again.\")\n",
    "        grading_output = chain.invoke(input=grading_input)\n",
    "    return(grading_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to extract the grading text and grading outcome in one step.\n",
    "def extract_info(df : pd.DataFrame, outcomeName : str, nItems = nRubricItems):\n",
    "    df.loc[:,f'{outcomeName}_text'] = df.loc[:,outcomeName].apply(lambda x:x.content)\n",
    "    df.loc[:,f'{outcomeName}_grade'] = df.loc[:,f'{outcomeName}_text'].apply(lambda x:binary_output.Extract_binary(x, nItems= nItems))\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This function automates the grading and extract of information process to avoid code copying errors.\n",
    "def do_grading(promptStyle : str, llm_chain, response_df : pd.DataFrame, name_append = '', nItems = nRubricItems):\n",
    "    colName = promptStyle if name_append == \"\" else f\"{promptStyle}_{name_append}\"\n",
    "    response_df[colName] = response_df['response'].apply(grade_by_llmChain, grading_input = grading_conditions_noparser[promptStyle], chain = llm_chain, nItems = nItems)\n",
    "    response_df = extract_info(response_df, colName, nItems = nItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test case with one answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomResponse = student_responses.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle= grading_condition_names[2], llm_chain=grading_chain_gpt35, response_df=randomResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response.print_gradingOutcome(randomResponse, grading_colName=\"detailed_compare_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test a random sample of 5 student answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_test_gpt35 = randomResponse_5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt35,response_df=grading_test_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grading_test_gpt35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response.print_gradingOutcome(grading_test_gpt35, grading_colName = f\"{grading_condition_names[2]}_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012586"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the price of this test run\n",
    "calc_price.Calc_Price(grading_test_gpt35[grading_condition_names[2]], modelUsed= 'gpt35')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grade all responses using GPT-35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this line only once as it creates a new variable\n",
    "#full_grading_gpt35 = student_responses.copy() #use copy to create a new variable with new variable id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[0], llm_chain=grading_chain_gpt35, response_df= full_grading_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[1], llm_chain=grading_chain_gpt35, response_df= full_grading_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt35, response_df= full_grading_gpt35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[3], llm_chain=grading_chain_gpt35, response_df= full_grading_gpt35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-3.5 typically takes about 2-3 minutes in grading all the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt35.to_csv('./data_chatAPI/FinalQ8/full_grading_gpt35_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23141399999999998"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_price.Calc_Price(full_grading_gpt35[grading_condition_names[3]], modelUsed=\"gpt35\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-35 took 9 minutes to grade and costed $0.19 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code below grades with gpt-4o model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup the llm connection with gpt-4o. \n",
    "#empirically, temperatur 0.8 seems to be a good balance. This is of course a guess. \n",
    "#only 4 stop sequences are allowed for Azure via API. This is an API issue. Under chat API this seems to not be a problem.\n",
    "gpt4_model = AzureChatOpenAI(    \n",
    "    api_version=\"2024-02-01\", #latest stable version\n",
    "    deployment_name = \"zchen-test-gpt-4o\",\n",
    "    max_tokens = 1000,\n",
    "    temperature= 0.8,\n",
    "    #model_version = '0301',\n",
    "    model_kwargs= {\"stop\": [\"<|im_end|>\", \"Student response:\", \"Grading:\", \"Example \"]}\n",
    "    )\n",
    "\n",
    "#define the grading chain. If parsing is needed, it can be connected to the output parser\n",
    "grading_chain_gpt4o = prompt_template_noformatting | gpt4_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grading_gpt4o = randomResponse_5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_cot_1', 'naive_cot_2', 'detailed_compare', 'forced_compare']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_condition_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt4o, response_df=test_grading_gpt4o)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response.print_gradingOutcome(test_grading_gpt4o, grading_colName=f'{grading_condition_names[2]}_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_grading_gpt4o.to_csv('./data_chatAPI/FinalQ8/grading_test_gpt4o.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneResponse = student_responses.iloc[[21]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(grading_condition_names[2], llm_chain= grading_chain_gpt4o, response_df= test_oneResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response.print_gradingOutcome(test_oneResponse, \"detailed_compare_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Despite our best effort of prompt engineering, GPT-4o refuse to acknowledge that the student considered the linear momentum of the second boulder separately, unless the exact form of the student's written formula was written in the first rubric explanation. Most likely the word \"decompose\" is strongly associated with trigonometric expression, and GPT would not recognize m2v2x and m2v2y as a valid form of considering the momentum components separately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_oneResponse.to_csv(\"./data_chatAPI/FinalQ8/test_oneCase.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save for reproducibility\n",
    "test_grading_gpt4o.to_csv(\"./data_chatAPI/FinalQ8/grading_test_gpt4o.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.046485"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate the cost of running\n",
    "#Note: I need a utility to calculate the price of multiple runs, and output a list\n",
    "calc_price.Calc_Price(test_grading_gpt4o['detailed_compare'], 'gpt4o')\n",
    "#calc_price.Calc_Price(test_grading_gpt4o['forced_compare'], 'gpt4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using GPT-4o to do full grading now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following line is run only once at the initiation of the process\n",
    "#full_grading_gpt4o = student_responses.copy() #Need to use the copy method to create a different variable, not an alias of the same variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[0], llm_chain=grading_chain_gpt4o, response_df= full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save to csv file\n",
    "full_grading_gpt4o.to_csv(\"./data_chatAPI/FinalQ8/full_grading_gpt4o_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.776285"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_price.Calc_Price(full_grading_gpt4o['detailed_compare'], modelUsed='gpt4o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-4o took 18 minutes to perform the grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[1], llm_chain=grading_chain_gpt4o, response_df= full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[2], llm_chain=grading_chain_gpt4o, response_df= full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(promptStyle=grading_condition_names[3], llm_chain=grading_chain_gpt4o, response_df= full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing improved detailed rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the improved rubric was tested on 5 cases in which the detailed compare grading with gpt-4o differed from both human raters, and with outcome of naive_COT_simple. The rubric explanation was iteratively improved until all 5 cases were graded satisfactorily by detailed compare. The main improvement focused on explaining items 2 and 3. For item 2, the requirement to \"explicitly write equations for both x and y directions\" was softened. For item 3, the explanation states that pythagorean theorem cannot be used directly on the linear momentum of boulders 1 and 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_newRubric_grading = student_responses[student_responses['student'].isin([412565, 995849, 671085, 177232, 105981])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_newRubric_grading_2 = student_responses[student_responses['student'].isin([620513, 292286, 673497, 714773, 553249])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['naive_cot_1',\n",
       " 'naive_cot_2',\n",
       " 'detailed_compare',\n",
       " 'forced_compare',\n",
       " 'detailed_compare_2',\n",
       " 'detailed_compare_3']"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grading_condition_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(grading_condition_names[4], llm_chain = grading_chain_gpt4o, response_df= test_newRubric_grading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_response.print_gradingOutcome(test_newRubric_grading, grading_colName=f'{grading_condition_names[4]}_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(grading_condition_names[5], llm_chain = grading_chain_gpt4o, response_df= test_newRubric_grading)\n",
    "print_response.print_gradingOutcome(test_newRubric_grading, grading_colName=f'{grading_condition_names[5]}_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(promptStyle=grading_condition_names[6], llm_chain=grading_chain_gpt4o, response_df=test_newRubric_grading_2)\n",
    "print_response.print_gradingOutcome(test_newRubric_grading_2, \"naive_cot_3_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do several more rounds of grading with improved rubric and detailed compared with simple rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_grading(grading_condition_names[4], llm_chain = grading_chain_gpt4o, response_df = full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(grading_condition_names[5], llm_chain = grading_chain_gpt4o, response_df = full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(grading_condition_names[6], llm_chain = grading_chain_gpt4o, response_df = full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "do_grading(grading_condition_names[7], llm_chain = grading_chain_gpt4o, response_df = full_grading_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grading_gpt4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: more rubric explanation do not always improve performance, especially when naive COT performance is already high (i.e. GPT-4o model's interpretation of original rubric is close to human experts). The naive COT level performance is reached with a simple rubric addressing the shortcomings of original rubric + detailed compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self consistency using nct 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_consistency = student_responses.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n",
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "#do_grading(promptStyle=grading_condition_names[0], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_1\") #first run\n",
    "#do_grading(promptStyle=grading_condition_names[0], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_2\") #second run \n",
    "#do_grading(promptStyle=grading_condition_names[0], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_3\") #third run\n",
    "#do_grading(promptStyle=grading_condition_names[0], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_4\") #fourth run\n",
    "#do_grading(promptStyle=grading_condition_names[0], llm_chain=grading_chain_gpt4o, response_df= self_consistency, name_append=\"run_5\") #fifth run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_consistency.to_csv(f'./data_chatAPI/FinalQ8/self_consistency_nct1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_consistency_2 = student_responses.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proper grading output not found, re-do grading again.\n"
     ]
    }
   ],
   "source": [
    "#do_grading(promptStyle=grading_condition_names[7], llm_chain=grading_chain_gpt4o, response_df= self_consistency_2, name_append=\"run_1\") #first run\n",
    "#do_grading(promptStyle=grading_condition_names[7], llm_chain=grading_chain_gpt4o, response_df= self_consistency_2, name_append=\"run_2\") #second run \n",
    "#do_grading(promptStyle=grading_condition_names[7], llm_chain=grading_chain_gpt4o, response_df= self_consistency_2, name_append=\"run_3\") #third run\n",
    "#do_grading(promptStyle=grading_condition_names[7], llm_chain=grading_chain_gpt4o, response_df= self_consistency_2, name_append=\"run_4\") #fourth run\n",
    "#do_grading(promptStyle=grading_condition_names[7], llm_chain=grading_chain_gpt4o, response_df= self_consistency_2, name_append=\"run_5\") #fifth run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_consistency_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "self_consistency_2.to_csv(f'./data_chatAPI/FinalQ8/self_consistency_dc4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing variables using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['student_responses', 'randomResponse', 'randomResponse_5', 'prompt_components_dict', 'prompt_dict', 'prompt_template_noformatting', 'grading_conditions_noparser', 'grading_condition_names', 'do_grading', 'extract_info', 'test_oneResponse', 'grading_test_gpt35', 'full_grading_gpt35', 'full_grading_gpt4o', 'test_newRubric_grading', 'test_newRubric_grading_2', 'test_grading_gpt4o', 'self_consistency', 'self_consistency_2']\n"
     ]
    }
   ],
   "source": [
    "#use the globalVars parameter to allow the function to access the current global variables list.\n",
    "try:\n",
    "    pickle_tools.save_as_pickle(\n",
    "        variables=[student_responses, \n",
    "                   randomResponse, \n",
    "                   randomResponse_5, \n",
    "                   prompt_components_dict, \n",
    "                   prompt_dict, \n",
    "                   prompt_template_noformatting, \n",
    "                   grading_conditions_noparser, \n",
    "                   grading_condition_names,\n",
    "                   do_grading, \n",
    "                   extract_info,\n",
    "                   test_oneResponse,\n",
    "                   grading_test_gpt35,\n",
    "                   full_grading_gpt35,\n",
    "                   full_grading_gpt4o,\n",
    "                   test_newRubric_grading, \n",
    "                   test_newRubric_grading_2,\n",
    "                   test_grading_gpt4o,\n",
    "                   self_consistency,\n",
    "                   self_consistency_2\n",
    "                   ], \n",
    "        folderName = project_folder,\n",
    "        globalVars=globals())\n",
    "except IndexError as err:\n",
    "    print(err)\n",
    "\n",
    "print(pickled_varNames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
