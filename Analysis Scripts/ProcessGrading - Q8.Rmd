---
title: "Process Grading for Final Q8"
output: html_notebook
---


```{r}
library(tidyverse)
library(rstatix)
```
```{r}
#Data <- list()
#Funs <- list()
#Results <- list()
```

## Data processing functions here

```{r}
#function to process human grading datafile
Funs$Process_human <- function(df, colSel, outName){
  df %>% rowwise() %>%
  mutate(!!outName := c_across({{colSel}}) %>% #select all human grading columns.
           as.integer() %>% c() %>% #turn into an integer vector
           list() #make it into a list column
           ) %>% select(-{{colSel}}) %>% ungroup()
}
```


```{r}
Funs$Prep_input <- function(inputdf, prompt_style, promptShort, humandf){
  gradeCol_long = str_c(prompt_style, "_grade")
  gradeCol_short = str_c(promptShort, "_grade")
  inputdf %>% 
    select("student", "response", !!str_c(prompt_style, "_text"), {{gradeCol_long}}) %>%
    mutate("{promptShort}_grade" := Funs$str_to_int(.data[[gradeCol_long]])) %>% #turning the initial grading stored as strings into an integer vector.
    Funs$Compare_machine_human(machine_grade_col = gradeCol_short, human_df = humandf)
}
```


```{r}
Funs$str_to_int <- function(vecStr){
  vecStr %>% 
    str_remove_all("[{}\\s]+") %>% #remove white spaces, this probably won't happen
    str_split(",") %>% #split into three sub-strings
    map(as.integer) #turn into a list of integer vectors
}

# Definition of simple matching distance
Funs$Calc_SMD <- function(vec1, vec2){
  if (length(vec1) != length(vec2)) {
    return(NA_real_)
  }
  # Count matching positions
  matches <- sum(vec1 == vec2)
  # Total number of elements
  total_elements <- length(vec1)
  # Calculate Simple Matching Coefficient (SMC)
  smc <- matches / total_elements
  # Return Simple Matching Distance
  return(1 - smc)  
}

Funs$Calc_SMD_Vec <- function(gradingVec1, gradingVec2){
  map2_dbl(gradingVec1, gradingVec2, \(x,y) Funs$Calc_SMD(x, y))
}

Funs$Calc_QWK <- function(gradingVec1, gradingVec2){
  rating1 <- map_int(gradingVec1, sum)
  rating2 <- map_int(gradingVec2, sum)
  Metrics::ScoreQuadraticWeightedKappa(rating1, rating2)
}

#Get simple matching distance for each human grader
Funs$Compare_machine_human <- function(machine_df, machine_grade_col, human_df = Data$human_gradeing){
  machine_df %>%
    left_join(human_df, by = c("student")) %>%
    #left_join(human_df %>% select(-response), by = c("student")) %>%
    #The response column may have slightly different line return characters when import/exported multiple times in python. "response" was a safty measure anyways and no necessary.
    #left_join(human_df, by = c("student", "response")) %>% 
    mutate(
      h1_SMD = Funs$Calc_SMD_Vec(human1, .data[[machine_grade_col]]),
      h2_SMD = Funs$Calc_SMD_Vec(human2, .data[[machine_grade_col]]),
      diff_both = (h1_SMD > 0 & h2_SMD > 0),
      diff_one = (h1_SMD > 0 | h2_SMD >0)
    )
}

#Calculate F1 score for prediction
## Calculate F1 score for one rubric item
Funs$Calc_single_F1 <- function(item, grade_AI, grade_human){
  score_AI = grade_AI %>% map_int(~.[[item]])
  score_Human = grade_human %>% map_int(~.[[item]])
  MLmetrics::F1_Score(y_true = score_Human, y_pred = score_AI)
}

## Calculate Macro F1 score (mean) for all three rubrics.

Funs$Calc_Macro_F1 <- function(grade_AI, grade_human, nRubric = nRubricItem){
  F1_scores = tibble("item" = c(1:nRubric))
  F1_scores %>% mutate(
    F1 = map_dbl(item, ~Funs$Calc_single_F1(., grade_AI = grade_AI, grade_human = grade_human))
  ) %>% {mean(.$F1)}
}

#Summarize matching percentages, QWK, Macro F1, and fraction of cases when AI disagree with both humans by the same amount
Funs$Summarize_Compare <- function(compareOutcome, machine_grade_col, SMDCols = ends_with("SMD")){
  compareOutcome %>% summarise(
    across(all_of(SMDCols), list(avg_SMD = mean, frac_match = \(x) sum(x == 0)/length(x))),
    QWK_h1 = Funs$Calc_QWK(human1, {{machine_grade_col}}),
    QWK_h2 = Funs$Calc_QWK(human2, {{machine_grade_col}}),
    QWK_avg = mean(c(QWK_h1,QWK_h2)),
    Macro_F1_h1 =  Funs$Calc_Macro_F1(human1, {{machine_grade_col}}),
    Macro_F1_h2 =  Funs$Calc_Macro_F1(human2, {{machine_grade_col}}),
    Macro_F1_Avg = mean(c(Macro_F1_h1,Macro_F1_h2)),
    diff_both_human = (sum(diff_both)/length(diff_both)) %>% round(3)
  ) %>% 
    relocate(matches("_h[12]$"), .after = "diff_both_human")
}

```


```{r}
## This is to make the human gradings visible for manual comparison
Funs$View_grades <- function(processed_gradeDf, promptType){
  processed_gradeDf %>%
    select(student, response, str_c(promptType, "_text"), str_c(promptType, "_grade"), starts_with("human")) %>%
    mutate(across(starts_with("human"), .fns = \(x) map_chr(x, \(x) str_c(x, collapse = ","))))
}
```

```{r}
#This is used to evaluate the outcomes of self-consistency runs
Funs$Evaluate_SC <- function(SC_df, entropy_cutoff = 0, case_col = diff_both){
  SC_df %>% mutate(Test = as_factor(entropy > entropy_cutoff), Case = as_factor({{case_col}})) %>% 
    {caret::confusionMatrix(.$Test, .$Case, positive = "TRUE")} -> cfMatrix
  
  cfMatrix %>% broom::tidy() %>% #use the caret package for confusion Matrix, and broom to clean the output.
    mutate(estimate = round(estimate, 3)) %>% select(term, estimate) %>% filter(term %in% c("accuracy", "sensitivity", "specificity", "precision", "f1")) %>% #select necessary outputs
    pivot_wider(names_from = "term", values_from = "estimate") %>% mutate(across(-"f1", scales::label_percent(0.1))) %>% #transform into a wide table and turn decimals into percentages.label_percent returns a function with the given level of precision and other parameters.
    rename(`sensitivity(recall)` = sensitivity) %>%
    mutate(
      entropy = str_c(">", entropy_cutoff),
      need_review = (rowSums(cfMatrix$table)[[2]]/sum(cfMatrix$table)) %>% scales::label_percent(0.1)(),
      .before = accuracy
    ) 
}


Funs$Evaluate_SC_multi <- function(SC_df, entropy_cutoff_vec, case_col = diff_both){
   map(entropy_cutoff_vec, \(entropy) Funs$Evaluate_SC(SC_df, entropy)) %>%
    list_rbind()
}
```

## Load and process the human grading data

Note that the processing was done in Q8_human_grading_2.Rmd
```{r}
Data$human_Q8_processed <- Data$human_Q8_processed %>% rename(Response = response)
```


```{r}
Data$human_Q8_SMD <- Data$human_Q8_SMD %>% rename(Response = response)
Data$human_Q8_SMD 
```


## human to human baseline

```{r}
Results$human_baseline_Q8
```

## Load machine grading data

```{r}
#Data$gpt35 <- list()
#Data$gpt4 <- list()
```


```{r}
#Data$gpt35$Q8 <- list()
#Data$gpt4$Q8 <- list()
```


### GPT 3.5
```{r}
Data$gpt35$Q8$grading <- read_csv("./FinalQ8/full_grading_gpt35_new.csv", show_col_types = F)
```
```{r}
Data$gpt35$Q8$grading
```

```{r}
Data$gpt35$Q8$grading %>% Funs$Prep_input("naive_cot_2", "nct2", humandf = Data$human_Q8_processed) -> Data$gpt35$Q8$nct2_processed
Data$gpt35$Q8$grading %>% Funs$Prep_input("naive_cot_1", "nct1", humandf = Data$human_Q8_processed) -> Data$gpt35$Q8$nct1_processed
Data$gpt35$Q8$grading %>% Funs$Prep_input("detailed_compare", "dc", humandf = Data$human_Q8_processed) -> Data$gpt35$Q8$dc_processed
Data$gpt35$Q8$grading %>% Funs$Prep_input("forced_compare", "fc", humandf = Data$human_Q8_processed) -> Data$gpt35$Q8$fc_processed
```

### GPT 4o
```{r}
#Data$gpt4$Q8$test_15 = read_csv("./test_grading_gpt4.csv", show_col_types = F)
Data$gpt4$Q8$grading = read_csv("./FinalQ8/full_grading_gpt4o_new.csv", show_col_types = F)
```
```{r}
Data$gpt4$Q8$grading
```



```{r}
#Data$gpt4$test_15 %>% Funs$Prep_input("naive_cot_2", "nct2") -> Data$gpt4$test_15_processed
Data$gpt4$Q8$grading %>% Funs$Prep_input("naive_cot_2", "nct2", humandf = Data$human_Q8_processed) -> Data$gpt4$Q8$nct2_processed
Data$gpt4$Q8$grading %>% Funs$Prep_input("naive_cot_1", "nct1", humandf = Data$human_Q8_processed) -> Data$gpt4$Q8$nct1_processed
Data$gpt4$Q8$grading %>% Funs$Prep_input("detailed_compare", "dc", humandf = Data$human_Q8_processed) -> Data$gpt4$Q8$dc_processed
Data$gpt4$Q8$grading %>% Funs$Prep_input("forced_compare", "fc", humandf = Data$human_Q8_processed) -> Data$gpt4$Q8$fc_processed
Data$gpt4$Q8$grading %>% Funs$Prep_input("detailed_compare_2", "dc2", humandf = Data$human_Q8_processed) -> Data$gpt4$Q8$dc2_processed
Data$gpt4$Q8$grading %>% Funs$Prep_input("detailed_compare_3", "dc3", humandf = Data$human_Q8_processed) -> Data$gpt4$Q8$dc3_processed
Data$gpt4$Q8$grading %>% Funs$Prep_input("naive_cot_3", "nct3", humandf = Data$human_Q8_processed) -> Data$gpt4$Q8$nct3_processed
Data$gpt4$Q8$grading %>% Funs$Prep_input("detailed_compare_4", "dc4", humandf = Data$human_Q8_processed) -> Data$gpt4$Q8$dc4_processed
```

```{r}
Data$gpt4$Q8$dc_processed
```

## Compare with both humans and summarize

```{r}
#Results <- list(
#  gpt35 = list(),
#  gpt4 = list()
#)
```


### GPT-35 results

```{r}
#Results$gpt35$Q8 <- list()
```

Detailed Rubric Comparison

```{r}
Results$gpt35$Q8$dc_summary <- Data$gpt35$Q8$dc_processed %>% Funs$Summarize_Compare(machine_grade_col = dc_grade)
Results$gpt35$Q8$dc_summary
```

Naive COT with detailed rubric
```{r}
Results$gpt35$Q8$nct_2_Summary <- Data$gpt35$Q8$nct2_processed %>% Funs$Summarize_Compare(machine_grade_col = nct2_grade)
Results$gpt35$Q8$nct_2_Summary
```

Naive COT with simple rubric
```{r}
Results$gpt35$Q8$nct_1_Summary <- Data$gpt35$Q8$nct1_processed %>% Funs$Summarize_Compare(machine_grade_col = nct1_grade)
Results$gpt35$Q8$nct_1_Summary
```





Forced compare with detailed rubric
```{r}
Results$gpt35$Q8$fc_Summary <- Data$gpt35$Q8$fc_processed %>% Funs$Summarize_Compare(machine_grade_col = fc_grade)
Results$gpt35$Q8$fc_Summary
```


## GPT-4o results

Detailed Comparison

```{r}
Data$gpt4$Q8$dc_processed
```


```{r}
Data$gpt4$Q8$dc_processed %>% Funs$Summarize_Compare(machine_grade_col = dc_grade) -> Results$gpt4$Q8$dc_Summary

Results$gpt4$Q8$dc_Summary
```

Naive CoT with detailed rubric item and gpt-4o model
```{r}
Data$gpt4$Q8$nct2_processed %>% Funs$Summarize_Compare(machine_grade_col = nct2_grade) -> Results$gpt4$Q8$nct2_Summary

Results$gpt4$Q8$nct2_Summary
```

## Naive CoT with simple rubric item and gpt-4o model
```{r}
Data$gpt4$Q8$nct1_processed %>% Funs$Summarize_Compare(machine_grade_col = nct1_grade) -> Results$gpt4$Q8$nct1_Summary

Results$gpt4$Q8$nct1_Summary
```
Very strangely, naive chain of thought 1 had the best performance of all, close to human level agreement. This indicates the rubric explanation had significant drawbacks and were written poorly.

```{r}
Data$gpt4$Q8$fc_processed %>% Funs$Summarize_Compare(machine_grade_col = fc_grade) -> Results$gpt4$Q8$fc_Summary

Results$gpt4$Q8$fc_Summary
```

```{r}
Data$gpt4$Q8$dc2_processed %>% Funs$Summarize_Compare(machine_grade_col = dc2_grade) -> Results$gpt4$Q8$dc2_Summary

Results$gpt4$Q8$dc2_Summary
```

```{r}
Data$gpt4$Q8$dc3_processed %>% Funs$Summarize_Compare(machine_grade_col = dc3_grade) -> Results$gpt4$Q8$dc3_Summary

Results$gpt4$Q8$dc3_Summary
```
```{r}
Data$gpt4$Q8$nct3_processed %>% Funs$Summarize_Compare(machine_grade_col = nct3_grade) -> Results$gpt4$Q8$nct3_Summary

Results$gpt4$Q8$nct3_Summary
```

```{r}
Data$gpt4$Q8$dc4_processed %>% Funs$Summarize_Compare(machine_grade_col = dc4_grade) -> Results$gpt4$Q8$dc4_Summary

Results$gpt4$Q8$dc4_Summary
```
In summary, detailed compare 3 (minimal rubric, detailed compare) is comparable to nct 1, with slightly better diff_both_human count, and matches h2 more. but lower average macro F1. 

## Note: Need to analyze when GPT disagrees with human consensus.

```{r}
Data$gpt4$Q8$dc_processed %>% left_join(Data$human_Q8_SMD %>% select(student, humanSMD), by = "student") %>%
  filter(h2_SMD & h1_SMD > 0) %>% Funs$View_grades("detailed_compare") %>%
  left_join(Data$gpt4$Q8$nct1_processed %>% select (student, naive_cot_1_text, naive_cot_1_grade), by = "student")->  Results$gpt4$Q8$dc_diffFromHuman
```


```{r}
Results$gpt4$Q8$dc_diffFromHuman %>% write_csv("./FinalQ8/gpt4_dc_HumanDiff.csv")
```

```{r}
Data$gpt4$Q8$dc2_processed %>% left_join(Data$human_Q8_SMD %>% select(student, humanSMD), by = "student") %>%
  filter(h2_SMD > 0 & h1_SMD > 0) %>% Funs$View_grades("detailed_compare_2") %>%
  left_join(Data$gpt4$Q8$nct1_processed %>% select (student, naive_cot_1_text, naive_cot_1_grade), by = "student") ->  Results$gpt4$Q8$dc2_diffFromHuman
```


```{r}
Results$gpt4$Q8$dc2_diffFromHuman %>% write_csv("./FinalQ8/gpt4_dc2_HumanDiff.csv")
```


```{r}
Data$gpt4$Q8$nct1_processed %>% left_join(Data$human_Q8_SMD %>% select(student, humanSMD), by = "student") %>%
  filter(h2_SMD > 0 & h1_SMD > 0) %>% Funs$View_grades("naive_cot_1") ->  Results$gpt4$Q8$nct1_diffFromHuman
```

```{r}
Results$gpt4$Q8$nct1_diffFromHuman %>% write_csv("./FinalQ8/gpt4_nct1_HumanDiff.csv")
```


#Next steps: 1. Can we ask GPT to generate a confidence interval for its own grading? 2. Can we ask GPT to generate feedback according to students' response and the grading reasoning? (How do we check this?)

#Process self_consistency grading

```{r}
#Process Raw Grades from Multiple Runs
Funs$Organize_grades <- function(rawData, colEnding = "_grade"){
  rawData %>% select(student, ends_with("_grade")) %>% #extract only the grading column.
    pivot_longer(cols = ends_with("_grade"), names_to = "runs", values_to = "score") %>% #make it a long table
    mutate(runs = str_extract(runs, "(?<=run_)\\d")) #simplify "runs" into just the number.
}

#The output is a list with two dataframes
Funs$Process_grades <- function(processedData){
  rslt <- list()
  maxRun = processedData[["runs"]] %>% as.integer() %>% max()
  rslt$score_freq <- processedData %>% group_by(student) %>%
    count(score, name = "freq") %>% arrange(student, desc(freq)) #need to arrange by descending frequency
  rslt$score_consistency <- rslt$score_freq %>%
    summarise(mode_score = score[[1]], #the most frequent score
              variety = length(freq),  #the number of different scores
              entropy = -(sum(freq/maxRun*log(freq/maxRun,base = 2))/log(maxRun,2)) %>% round(2)) %>% #normalized Shannon Entropy of score.
    arrange(desc(entropy))
  return(rslt)
}


Funs$Sum_oneRun <- function(runName, promptStyle, grading_df, humandf){
  #prepare the proper column names for summary functions
  runColName = str_c(promptStyle, "_", runName)
  runRsltName = str_c(runName, "_grade")
  #invoke the summary functions and make some modifications
  grading_df %>% Funs$Prep_input(runColName, runName, humandf = humandf) %>%
    Funs$Summarize_Compare(.[[runRsltName]]) %>% select(-matches("_h[12]$")) %>%
    pivot_longer(everything(),names_to = "data_type", values_to = "value") %>%
    mutate(value = round(value, 2))
}

Funs$Sum_multiRuns <- function(grading_df, promptStyle, human_df, nRuns = 5){
  outputTbl <- tibble(runs = paste0("run_", c(1:nRuns)))
  outputTbl %>% mutate(summary = map(runs, ~Funs$Sum_oneRun(.,promptStyle, grading_df, humandf = human_df))) %>% 
    unnest(summary) %>% 
    group_by(data_type) %>% summarise(mean = mean(value), min = min(value), max = max(value))
}
```


```{r}
#Data$gpt4$self_consistency <- list()
```

```{r}
#Data$gpt4$Q8$self_consistency <- list()
```

```{r}
Data$gpt4$Q8$self_consistency$raw <- read_csv("./FinalQ8/self_consistency_nct1.csv")
Data$gpt4$Q8$self_consistency_2$raw <- read_csv("./FinalQ8/self_consistency_dc4.csv")
#Data$gpt4$Q8$self_consistency_raw %>% select(studentId, response, outcome, !contains("detailed_compare_2")) %>% rename(student = studentId) -> Data$gpt4$Q10$self_consistency$raw
#Data$gpt4$Q8$self_consistency_raw %>% select(studentId, response, outcome, contains("detailed_compare_2")) %>% rename(student = studentId) -> Data$gpt4$Q10$self_consistency_2$raw

```


```{r}
#Note this line of code is no longer needed since I updated self_consistency_grading.csv.
#Data$gpt4$Q10$self_consistency$raw <- read_csv("./Final_Q10/self_consistency_grading.csv")
```
```{r}
Data$gpt4$Q8$self_consistency$raw %>% Funs$Organize_grades() %>%
  Funs$Process_grades() -> Results$gpt4$Q8$self_consistency
```


```{r}
Results$gpt4$Q8$self_consistency$score_consistency %>% 
  mutate(mode_score = Funs$str_to_int(mode_score)) %>%
  Funs$Compare_machine_human(machine_grade_col = "mode_score", human_df = Data$human_Q8_processed) -> Results$gpt4$Q8$sc_compare
```


```{r}
Results$gpt4$Q8$sc_compare %>% Funs$Summarize_Compare(machine_grade_col = mode_score) -> Results$gpt4$Q8$sc_summary
Results$gpt4$Q8$sc_summary
```


```{r}
Data$gpt4$Q8$self_consistency$raw %>% Funs$Sum_multiRuns(promptStyle = "naive_cot_1", human_df = Data$human_Q8_processed) -> Results$gpt4$Q8$self_consistency$outcome_summary
Results$gpt4$Q8$self_consistency$outcome_summary
```


```{r}
Data$gpt4$Q8$self_consistency_2$raw %>% Funs$Organize_grades() %>%
  Funs$Process_grades() -> Results$gpt4$Q8$self_consistency_2
```


```{r}
Results$gpt4$Q8$self_consistency_2$score_consistency %>% 
  mutate(mode_score = Funs$str_to_int(mode_score)) %>%
  Funs$Compare_machine_human(machine_grade_col = "mode_score", human_df = Data$human_Q8_processed) -> Results$gpt4$Q8$sc_2_compare
```


```{r}
Results$gpt4$Q8$sc_2_compare %>% Funs$Summarize_Compare(machine_grade_col = mode_score) -> Results$gpt4$Q8$sc_2_summary
Results$gpt4$Q8$sc_2_summary
```

```{r}
Data$gpt4$Q8$self_consistency_2$raw %>% Funs$Sum_multiRuns(promptStyle = "detailed_compare_4", human_df = Data$human_Q8_processed) -> Results$gpt4$Q8$self_consistency_2$outcome_summary
Results$gpt4$Q8$self_consistency_2$outcome_summary
```




Accuracy: $\frac{TP + TN}{TP+TN+FP+FN}$ Correctly identified cases among all cases (but this is highly umbalanced)
Sensitivity (same as recall): $\frac{TP}{TP + FN}$ Correctly identified positive among all positive cases. (most relevant)
Specificity: $\frac{TN}{TN + FP}$ Correctly classified negative cases among all negative cases. 
Precision: $\frac{TP}{TP+FP}$ Correctly identified positives among all identified cases. (how efficient is the test)

```{r}
Results$gpt4$Q8$sc_compare %>% Funs$Evaluate_SC_multi(c(0, 0.4)) -> Results$gpt4$Q8$self_consistency$sc_test_summary
Results$gpt4$Q8$self_consistency$sc_test_summary
```
```{r}
Results$gpt4$Q8$sc_2_compare %>% Funs$Evaluate_SC_multi(c(0, 0.4)) -> Results$gpt4$Q8$self_consistency_2$sc_test_summary
Results$gpt4$Q8$self_consistency_2$sc_test_summary
```
Since the variation is smaller, dc4 is a more practical test. Reviewing 17% of the cases, and identify about 39% of the all problematic cases, which leaves about 8% error rate. 



## Need to write function to calculate the summary for all 5 runs, and compare the results to the self-consistency outcome. 


```{r}

```



```{r}
Data$gpt4$Q10$self_consistency_2$raw %>% Funs$Sum_multiRuns(promptStyle = "detailed_compare_2", human_df = Data$human_gradeing_Q10) -> Results$gpt4$Q10$self_consistency_2$outcome_summary
Results$gpt4$Q10$self_consistency_2$outcome_summary
```


```{r}
Results$Tables$Question3$gpt4o$raw_data
```
```{r}
Data$human_gradeing_SMD
```

