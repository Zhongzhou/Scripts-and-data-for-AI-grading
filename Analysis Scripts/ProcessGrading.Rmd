---
title: "Process Grading"
output: html_notebook
---


```{r}
library(tidyverse)
library(rstatix)
```
```{r}
#Data <- list()
#Funs <- list()
#Results <- list()
```

## Data processing functions here

```{r}
#Global Variable
nRubricItem = 3
```


```{r}
Funs$Prep_input <- function(inputdf, prompt_style, promptShort, humandf){
  gradeCol_long = str_c(prompt_style, "_grade")
  gradeCol_short = str_c(promptShort, "_grade")
  inputdf %>% 
    select("student", "response", !!str_c(prompt_style, "_text"), {{gradeCol_long}}) %>%
    mutate("{promptShort}_grade" := Funs$str_to_int(.data[[gradeCol_long]])) %>%
    Funs$Compare_machine_human(machine_grade_col = gradeCol_short, human_df = humandf)
}
```


```{r}
Funs$str_to_int <- function(vecStr){
  vecStr %>% 
    str_remove_all("[{}\\s]+") %>% #remove white spaces, this probably won't happen
    str_split(",") %>% #split into three sub-strings
    map(as.integer) #turn into a list of integer vectors
}

# Definition of simple matching distance
Funs$Calc_SMD <- function(vec1, vec2){
  if (length(vec1) != length(vec2)) {
    return(NA_real_)
  }
  # Count matching positions
  matches <- sum(vec1 == vec2)
  # Total number of elements
  total_elements <- length(vec1)
  # Calculate Simple Matching Coefficient (SMC)
  smc <- matches / total_elements
  # Return Simple Matching Distance
  return(1 - smc)  
}

Funs$Calc_SMD_Vec <- function(gradingVec1, gradingVec2){
  map2_dbl(gradingVec1, gradingVec2, \(x,y) Funs$Calc_SMD(x, y))
}

Funs$Calc_QWK <- function(gradingVec1, gradingVec2){
  rating1 <- map_int(gradingVec1, sum)
  rating2 <- map_int(gradingVec2, sum)
  Metrics::ScoreQuadraticWeightedKappa(rating1, rating2)
}

#Get simple matching distance for each human grader
Funs$Compare_machine_human <- function(machine_df, machine_grade_col, human_df = Data$human_gradeing){
  machine_df %>%
    left_join(human_df %>% select(-response), by = c("student")) %>%
    #The response column may have slightly different line return characters when import/exported multiple times in python. "response" was a safty measure anyways and no necessary.
    #left_join(human_df, by = c("student", "response")) %>% 
    mutate(
      h1_SMD = Funs$Calc_SMD_Vec(human1, .data[[machine_grade_col]]),
      h2_SMD = Funs$Calc_SMD_Vec(human2, .data[[machine_grade_col]]),
      diff_both = (h1_SMD > 0 & h2_SMD > 0),
      diff_one = (h1_SMD > 0 | h2_SMD >0)
    )
}

#Calculate F1 score for prediction
## Calculate F1 score for one rubric item
Funs$Calc_single_F1 <- function(item, grade_AI, grade_human){
  score_AI = grade_AI %>% map_int(~.[[item]])
  score_Human = grade_human %>% map_int(~.[[item]])
  MLmetrics::F1_Score(y_true = score_Human, y_pred = score_AI)
}

## Calculate Macro F1 score (mean) for all three rubrics.

Funs$Calc_Macro_F1 <- function(grade_AI, grade_human, nRubric = nRubricItem){
  F1_scores = tibble("item" = c(1:nRubric))
  F1_scores %>% mutate(
    F1 = map_dbl(item, ~Funs$Calc_single_F1(., grade_AI = grade_AI, grade_human = grade_human))
  ) %>% {mean(.$F1)}
}

#Summarize matching percentages, QWK, Macro F1, and fraction of cases when AI disagree with both humans by the same amount
Funs$Summarize_Compare <- function(compareOutcome, machine_grade_col, SMDCols = ends_with("SMD")){
  compareOutcome %>% summarise(
    across(all_of(SMDCols), list(avg_SMD = mean, frac_match = \(x) sum(x == 0)/length(x))),
    QWK_h1 = Funs$Calc_QWK(human1, {{machine_grade_col}}),
    QWK_h2 = Funs$Calc_QWK(human2, {{machine_grade_col}}),
    QWK_avg = mean(c(QWK_h1,QWK_h2)),
    Macro_F1_h1 =  Funs$Calc_Macro_F1(human1, {{machine_grade_col}}),
    Macro_F1_h2 =  Funs$Calc_Macro_F1(human2, {{machine_grade_col}}),
    Macro_F1_Avg = mean(c(Macro_F1_h1,Macro_F1_h2)),
    diff_both_human = (sum(diff_both)/length(diff_both)) %>% round(3)
  ) %>% 
    relocate(matches("_h[12]$"), .after = "diff_both_human")
}

```

```{r}
## This is to make the human gradings visible for manual comparison
Funs$View_grades <- function(processed_gradeDf, promptType){
  processed_gradeDf %>%
    select(student, response, str_c(promptType, "_text"), str_c(promptType, "_grade"), starts_with("human")) %>%
    mutate(across(starts_with("human"), .fns = \(x) map_chr(x, \(x) str_c(x, collapse = ","))))
}
```

```{r}
#This is used to evaluate the outcomes of self-consistency runs
Funs$Evaluate_SC <- function(SC_df, entropy_cutoff = 0, case_col = diff_both){
  SC_df %>% mutate(Test = as_factor(entropy > entropy_cutoff), Case = as_factor({{case_col}})) %>% 
    {caret::confusionMatrix(.$Test, .$Case, positive = "TRUE")} -> cfMatrix
  
  cfMatrix %>% broom::tidy() %>% #use the caret package for confusion Matrix, and broom to clean the output.
    mutate(estimate = round(estimate, 3)) %>% select(term, estimate) %>% filter(term %in% c("accuracy", "sensitivity", "specificity", "precision", "f1")) %>% #select necessary outputs
    pivot_wider(names_from = "term", values_from = "estimate") %>% mutate(across(-"f1", scales::label_percent(0.1))) %>% #transform into a wide table and turn decimals into percentages.label_percent returns a function with the given level of precision and other parameters.
    rename(`sensitivity(recall)` = sensitivity) %>%
    mutate(
      entropy = str_c(">", entropy_cutoff),
      need_review = (rowSums(cfMatrix$table)[[2]]/sum(cfMatrix$table)) %>% scales::label_percent(0.1)(),
      .before = accuracy
    ) 
}
```

## Load the human grading data
```{r}
Data$human_gradeing <- readRDS("../data/human_grading.Rds")
```

## human to human baseline

```{r}
Data$human_gradeing %>% mutate(
  humanSMD = Funs$Calc_SMD_Vec(human1, human2)
) -> Data$human_gradeing_SMD

Results$human_baseline <- Data$human_gradeing_SMD %>% summarise(
    avg_SMD = mean(humanSMD), 
    frac_match =  sum(humanSMD == 0)/length(humanSMD),
    QWK = Funs$Calc_QWK(human1, human2),
    F1 = Funs$Calc_Macro_F1(human1, human2)
  )
```

```{r}
Results$human_baseline
```

## Load machine grading data

```{r}
#Data$gpt35 <- list()
#Data$gpt4 <- list()
```

### GPT 3.5
```{r}
Data$gpt35$grading = read_csv("full_grading.csv", show_col_types = F)
```

```{r}
Data$gpt35$grading %>% Funs$Prep_input("naive_cot_2", "nct2", humandf = Data$human_gradeing) -> Data$gpt35$nct2_processed
Data$gpt35$grading %>% Funs$Prep_input("naive_cot_1", "nct1", humandf = Data$human_gradeing) -> Data$gpt35$nct1_processed
Data$gpt35$grading %>% Funs$Prep_input("forced_compare", "fc", humandf = Data$human_gradeing) -> Data$gpt35$fc_processed
Data$gpt35$grading %>% Funs$Prep_input("detailed_compare", "dc", humandf = Data$human_gradeing) -> Data$gpt35$dc_processed
```

Need to do GPT-35 dc.

### GPT 4o
```{r}
#Data$gpt4$test_15 = read_csv("./test_grading_gpt4.csv", show_col_types = F)
Data$gpt4$grading = read_csv("./full_grading_gpt4o.csv", show_col_types = F)
```
```{r}
#Data$gpt4$test_15 %>% Funs$Prep_input("naive_cot_2", "nct2") -> Data$gpt4$test_15_processed
Data$gpt4$grading %>% Funs$Prep_input("naive_cot_2", "nct2", humandf = Data$human_gradeing) -> Data$gpt4$nct2_processed
Data$gpt4$grading %>% Funs$Prep_input("naive_cot_1", "nct1", humandf = Data$human_gradeing) -> Data$gpt4$nct1_processed
Data$gpt4$grading %>% Funs$Prep_input("detailed_compare", "dc", humandf = Data$human_gradeing) -> Data$gpt4$dc_processed
Data$gpt4$grading %>% Funs$Prep_input("forced_compare", "fc", humandf = Data$human_gradeing) -> Data$gpt4$fc_processed
```


## Compare with both humans and summarize

```{r}
#Results <- list(
#  gpt35 = list(),
#  gpt4 = list()
#)
```


### GPT-35 results

Naive COT with detailed rubric
```{r}
Results$gpt35$nct_2_Summary <- Data$gpt35$nct2_processed %>% Funs$Summarize_Compare(machine_grade_col = nct2_grade)
Results$gpt35$nct_2_Summary
```
Naive COT with simple rubric
```{r}
Results$gpt35$nct_1_Summary <- Data$gpt35$nct1_processed %>% Funs$Summarize_Compare(machine_grade_col = nct1_grade)
Results$gpt35$nct_1_Summary
```

Forced compare with detailed rubric
```{r}
Results$gpt35$fc_Summary <- Data$gpt35$fc_processed %>% Funs$Summarize_Compare(machine_grade_col = fc_grade)
Results$gpt35$fc_Summary
```

```{r}
Results$gpt35$dc_Summary <- Data$gpt35$dc_processed %>% Funs$Summarize_Compare(machine_grade_col = dc_grade)
Results$gpt35$dc_Summary
```


## test gpt-4o model with 15 answers
```{r}
Results$gpt4$test15_nct_2_Summary <- Data$gpt4$test_15_processed %>% Funs$Summarize_Compare(machine_grade_col = nct2_grade)

Results$gpt4$test15_nct_2_Summary
```

## Naive CoT with detailed rubric item and gpt-4o model
```{r}
Data$gpt4$nct2_processed %>% Funs$Summarize_Compare(machine_grade_col = nct2_grade) -> Results$gpt4$nct2_Summary

Results$gpt4$nct2_Summary
```

## Naive CoT with simple rubric item and gpt-4o model
```{r}
Data$gpt4$nct1_processed %>% Funs$Summarize_Compare(machine_grade_col = nct1_grade) -> Results$gpt4$nct1_Summary

Results$gpt4$nct1_Summary
```


## gpt4 detaild compare
```{r}
Data$gpt4$dc_processed %>% Funs$Summarize_Compare(machine_grade_col = dc_grade) -> Results$gpt4$dc_Summary

Results$gpt4$dc_Summary
```

detailed compare reduced the differences in both humans from 16% to 15%, which is just one case. It is actually slightly cheapter than naive_cot_2 for some reason. Performance is identical in this case.

## gpt4 forced compare

```{r}
Data$gpt4$fc_processed %>% Funs$Summarize_Compare(machine_grade_col = fc_grade) -> Results$gpt4$fc_Summary

Results$gpt4$fc_Summary
```

```{r}
Data$gpt4$grading
```



#Process self_consistency grading
```{r}
#Data$gpt4$self_consistency <- list()
```

```{r}
#Process Raw Grades from Multiple Runs
Funs$Organize_grades <- function(rawData, colEnding = "_grade"){
  rawData %>% select(student, ends_with("_grade")) %>% #extract only the grading column.
    pivot_longer(cols = ends_with("_grade"), names_to = "runs", values_to = "score") %>% #make it a long table
    mutate(runs = str_extract(runs, "(?<=run_)\\d")) #simplify "runs" into just the number.
}

#The output is a list with two dataframes
Funs$Process_grades <- function(processedData){
  rslt <- list()
  maxRun = processedData[["runs"]] %>% as.integer() %>% max()
  rslt$score_freq <- processedData %>% group_by(student) %>%
    count(score, name = "freq") %>% arrange(student, desc(freq)) #need to arrange by descending frequency
  rslt$score_consistency <- rslt$score_freq %>%
    summarise(mode_score = score[[1]], #the most frequent score
              variety = length(freq),  #the number of different scores
              entropy = -(sum(freq/maxRun*log(freq/maxRun,base = 2))/log(maxRun,2)) %>% round(2)) %>% #normalized Shannon Entropy of score.
    arrange(desc(entropy))
  return(rslt)
}
```


```{r}
Data$gpt4$self_consistency$raw <- read_csv("../self_consistency/Conceptual_1/detailed_compare_runs.csv")
```
```{r}
Data$gpt4$self_consistency$raw %>% Funs$Organize_grades() %>%
  Funs$Process_grades() -> Results$gpt4$self_consistency
```



```{r}
Results$gpt4$self_consistency$score_consistency %>% 
  mutate(mode_score = Funs$str_to_int(mode_score)) %>%
  Funs$Compare_machine_human(machine_grade_col = "mode_score", human_df = Data$human_gradeing) -> Results$gpt4$sc_compare
```

```{r}
Results$gpt4$sc_compare %>% Funs$Summarize_Compare(machine_grade_col = mode_score) -> Results$gpt4$sc_summary
```

```{r}
Results$gpt4$sc_summary
```

nct2 has 15 cases of different from both human, dc has 14 cases, self-consistency dc has 13 cases. Improvement is marginal since it is pretty much topped out.



What percentage of cases where GPT differs from both humans is captured in high entropy cases?

AI differs from both humans as Positive Cases.
High entropy (S > 0) as Diagnosed Positive Cases.

```{r}
Results$gpt4$sc_compare %>% filter(h1_SMD > 0, h2_SMD > 0)
```

```{r}
Results$gpt4$sc_compare %>% filter(h1_SMD > 0, h2_SMD > 0) %>% select(student, variety, mode_score) %>% left_join(Data$gpt4$dc_processed, by = "student") %>% select(student, variety, response, detailed_compare_grade, mode_score, detailed_compare_text) %>% mutate(mode_score = map_chr(mode_score, ~str_flatten_comma(.))) %>% write_csv("./diff_both_sc.csv")
```
Note: Upon examining the items where the AI grading differs from human grading, there doesn't seem to be a clear and consistent pattern where AI grading is consistently less accurate or less valid, due to a problem in the rubric design. Therefore we decided not to improve the rubric, also the level of agreement between human graders is almost identical to the level of agreement between human and AI, so we concluded that further improvement of rubric is not necessary in this case.

```{r}
#Results$gpt4$self_consistency %>% count(entropy > 0)
Results$gpt4$self_consistency %>% filter(entropy > 0) %>% count(h1_SMD > 0 | h2_SMD > 0)
Results$gpt4$self_consistency %>% filter(entropy == 0) %>% count(h1_SMD > 0 | h2_SMD > 0)
```

```{r}
Results$gpt4$self_consistency %>% count(entropy > 0)
Results$gpt4$self_consistency %>% filter(entropy > 0.3)
```



13 cases (13.5%) where GPT grading differs from both humans raters. 9 of which (70%) had entropy > 0, 6 of which (46%) had entropy > 0.3. Therefore, the overall accuracy of the process can reach 96% (92/96) if entropy > 0 is reviewed, and 93% (89/96) if entropy > 0.4 is reviewed, which is only 17 out of 96 cases, or about 1/5 of the overall grading. 

Of the 26 cases where entropy > 0, 22 of which the rating differs with at least one human (85%) (precision is fine)

Of the 70 cases where entropy = 0, 13 of which the rating differs with at least one human (18.6%)


Write a function to calculate these as a function of the cutoff entropy, and whether it differs with one human or two humans.  

Accuracy: $\frac{TP + TN}{TP+TN+FP+FN}$ Correctly identified cases among all cases (but this is highly umbalanced)
Sensitivity (same as recall): $\frac{TP}{TP + FN}$ Correctly identified positive among all positive cases. (most relevant)
Specificity: $\frac{TN}{TN + FP}$ Correctly classified negative cases among all negative cases. 
Precision: $\frac{TP}{TP+FP}$ Correctly identified positives among all identified cases. (how efficient is the test)

```{r}
Results$gpt4$sc_compare %>% Funs$Evaluate_SC_multi(c(0, 0.4)) -> Results$gpt4$self_consistency$sc_test_summary
Results$gpt4$self_consistency$sc_test_summary
```
```{r}
Results$gpt4$self_consistency %>% Funs$Evaluate_SC(entropy_cutoff = 0.4)
```
```{r}
Results$gpt4$self_consistency %>% Funs$Evaluate_SC(entropy_cutoff = 0.45)
```
When 30% of the cases are reviewed, then 70% of the problematic cases can be captured.

When 18% of the cases are reviewed, then 46% of the problematic cases can be captured.

Further reducing the fraction of cases reviewed significantly reduces the sensitivity of the test. 

```{r}
Results$gpt4$self_consistency %>% left_join(Data$human_gradeing_SMD %>% select(student, humanSMD), by = "student") %>%
  mutate(diff_agree = (h1_SMD > 0 & humanSMD == 0)) %>% Funs$Evaluate_SC(entropy_cutoff = 0, case_col = diff_agree)
```
```{r}
Results$gpt4$self_consistency %>% mutate(Test = as_factor(entropy > 0), Case = as_factor(diff_both)) %>% 
    {caret::confusionMatrix(.$Test, .$Case, positive = "TRUE")} -> test
```




## Need to write function to calculate the summary for all 5 runs, and compare the results to the self-consistency outcome. 

```{r}
Funs$Sum_oneRun <- function(runName, promptStyle, grading_df, humandf){
  #prepare the proper column names for summary functions
  runColName = str_c(promptStyle, "_", runName)
  runRsltName = str_c(runName, "_grade")
  #invoke the summary functions and make some modifications
  grading_df %>% Funs$Prep_input(runColName, runName, humandf = humandf) %>%
    Funs$Summarize_Compare(.[[runRsltName]]) %>% select(-matches("_h[12]$")) %>%
    pivot_longer(everything(),names_to = "data_type", values_to = "value") %>%
    mutate(value = round(value, 3))
}

Funs$Sum_multiRuns <- function(grading_df, promptStyle, human_df, nRuns = 5){
  outputTbl <- tibble(runs = paste0("run_", c(1:nRuns)))
  outputTbl %>% mutate(summary = map(runs, ~Funs$Sum_oneRun(.,promptStyle, grading_df, humandf = human_df))) %>% 
    unnest(summary) %>% 
    group_by(data_type) %>% summarise(mean = round(mean(value),3), min = min(value), max = max(value))
}
```

```{r}
Data$gpt4$self_consistency$raw %>% Funs$Sum_multiRuns(promptStyle = "detailed_compare", human_df = Data$human_gradeing) -> Results$gpt4$self_consistency$outcome_summary
Results$gpt4$self_consistency$outcome_summary
```

```{r}
Results$gpt4$sc_summary
```
```{r}
Results$human_baseline
```

